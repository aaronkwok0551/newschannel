# -*- coding: utf-8 -*-
import streamlit as st
import requests
import feedparser
import datetime
import pytz
import urllib.parse
import time
from bs4 import BeautifulSoup
import sys
from streamlit_autorefresh import st_autorefresh

# è¨­å®šé è¨­ç·¨ç¢¼
try:
    sys.stdout.reconfigure(encoding='utf-8')
except:
    pass

# --- 1. é é¢è¨­å®š ---
st.set_page_config(
    page_title="Tommy Sir å¾Œæ´æœƒä¹‹æ–°èç›£å¯Ÿç³»çµ±",
    page_icon="ğŸ“°",
    layout="wide"
)

# è‡ªå‹•åˆ·æ–° (æ¯ 60 ç§’)
st_autorefresh(interval=60 * 1000, limit=None, key="news_autoupdate")

# --- CSS æ¨£å¼ (Clean Style - ç„¡æ¡†ç·šæ¸…å–®é¢¨æ ¼) ---
st.markdown("""
<style>
    /* å…¨å±€èƒŒæ™¯ */
    .stApp { background-color: #f8fafc; }

    /* é–ƒçˆç‰¹æ•ˆ - é‡å° 20 åˆ†é˜å…§çš„æ–°è */
    @keyframes blinker { 50% { opacity: 0.4; } }
    .new-badge {
        color: #ef4444;
        font-weight: 800;
        animation: blinker 1.5s ease-in-out infinite;
        margin-right: 5px;
        font-size: 0.75em;
    }
    
    /* å·²è®€ç‹€æ…‹ */
    .read-text {
        color: #94a3b8 !important;
        font-weight: normal !important;
        text-decoration: none !important;
    }
    
    /* é€£çµæ¨£å¼ */
    a { text-decoration: none; color: #334155; font-weight: 600; transition: 0.2s; font-size: 0.95em; line-height: 1.4; }
    a:hover { color: #2563eb; }
    
    /* ä¾†æºæ¨™é¡Œ (å¡ç‰‡é ­éƒ¨) */
    .news-source-header { 
        font-size: 1rem; 
        font-weight: bold; 
        color: #1e293b; 
        padding: 12px 15px;
        background-color: #ffffff;
        border-bottom: 2px solid #f1f5f9;
        border-top-left-radius: 10px;
        border-top-right-radius: 10px;
        display: flex;
        justify-content: space-between;
        align-items: center;
        box-shadow: 0 1px 2px rgba(0,0,0,0.02);
    }
    
    /* ç‹€æ…‹æ¨™ç±¤ */
    .status-badge {
        font-size: 0.65em;
        padding: 2px 8px;
        border-radius: 12px;
        font-weight: 500;
        background-color: #f1f5f9;
        color: #64748b;
    }
    
    /* æ–°èé …ç›®åˆ—è¡¨æ¨£å¼ (Clean Style) */
    .news-list-container {
        background-color: #ffffff;
        border-bottom-left-radius: 10px;
        border-bottom-right-radius: 10px;
        box-shadow: 0 1px 3px rgba(0,0,0,0.05);
        border: 1px solid #e2e8f0;
        border-top: none;
        padding-bottom: 5px;
        height: 100%;
    }

    .news-item-row {
        padding: 8px 12px;
        border-bottom: 1px solid #f1f5f9;
        transition: background-color 0.1s;
    }
    .news-item-row:hover {
        background-color: #f8fafc;
    }
    .news-item-row:last-child {
        border-bottom: none;
    }
    
    /* Checkbox å¾®èª¿ */
    .stCheckbox { margin-bottom: 0px; margin-top: 2px; }
    div[data-testid="column"] { display: flex; align-items: start; }
    
    /* èª¿æ•´ Expander/Dialog æ¨£å¼ */
    div[data-testid="stDialog"] { border-radius: 15px; }
    
    /* ç”Ÿæˆå…§å®¹å€åŸŸæ¨£å¼ */
    .generated-box {
        border: 2px solid #3b82f6;
        border-radius: 12px;
        padding: 20px;
        background-color: #ffffff;
        box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);
        margin-bottom: 20px;
    }
</style>
""", unsafe_allow_html=True)

# è¨­å®šæ™‚å€
HK_TZ = pytz.timezone('Asia/Hong_Kong')
UTC_TZ = pytz.timezone('UTC')

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
}

# --- 2. æ ¸å¿ƒåŠŸèƒ½å‡½å¼ ---

def chunked(lst, n):
    """å°‡åˆ—è¡¨åˆ†å‰²ç‚ºå¤§å°ç‚º n çš„å¡Š"""
    return [lst[i:i + n] for i in range(0, len(lst), n)]

def fetch_full_article(url):
    """ æŠ“å–æ–°èæ­£æ–‡ """
    try:
        r = requests.get(url, headers=HEADERS, timeout=6)
        r.encoding = 'utf-8'
        soup = BeautifulSoup(r.text, 'html.parser')
        
        for tag in soup(['script', 'style', 'header', 'footer', 'nav', 'iframe', 'noscript']):
            tag.decompose()

        # æ™ºæ…§æŠ“å–
        content_area = soup.find('div', class_=lambda x: x and ('article' in x.lower() or 'content' in x.lower() or 'news-text' in x.lower()))
        
        if content_area:
            paragraphs = content_area.find_all(['p', 'div'], recursive=False)
        else:
            paragraphs = soup.find_all('p')

        if not paragraphs:
            return "(ç„¡æ³•è‡ªå‹•æå–å…¨æ–‡ï¼Œè«‹é»æ“Šé€£çµæŸ¥çœ‹ç¶²é ç‰ˆ)"
            
        full_text = "\n".join([p.get_text().strip() for p in paragraphs if len(p.get_text().strip()) > 5])
        return full_text if len(full_text) > 30 else "(å…§å®¹éçŸ­æˆ–å—é™)"
    except:
        return "(å…¨æ–‡æŠ“å–å¤±æ•—)"

def resolve_google_url(url):
    """ é‚„åŸ Google News çœŸå¯¦é€£çµ """
    if "news.google.com" not in url:
        return url
    try:
        r = requests.head(url, headers=HEADERS, allow_redirects=True, timeout=5)
        return r.url
    except:
        return url

def is_new_news(time_str):
    """ åˆ¤æ–·æ˜¯å¦ç‚º 20 åˆ†é˜å…§çš„æ–°è """
    try:
        if time_str == "æœ€æ–°": return True 
        pub_time = datetime.datetime.strptime(time_str, '%Y-%m-%d %H:%M')
        pub_time = HK_TZ.localize(pub_time)
        now = datetime.datetime.now(HK_TZ)
        diff = (now - pub_time).total_seconds() / 60
        return 0 <= diff <= 20
    except:
        return False

# --- 3. é›™é‡ä¿éšªæŠ“å–æ©Ÿåˆ¶ ---

def fetch_google_proxy(site_query, site_name, color):
    """ Plan B: Google News ä»£ç†æ¨¡å¼ """
    query = urllib.parse.quote(site_query)
    rss_url = f"https://news.google.com/rss/search?q={query}+when:1d&hl=zh-HK&gl=HK&ceid=HK:zh-Hant"
    
    try:
        feed = feedparser.parse(rss_url)
        news_list = []
        for entry in feed.entries[:10]:
            title = entry.title.rsplit(" - ", 1)[0]
            dt_str = "æœ€æ–°"
            if hasattr(entry, 'published_parsed'):
                dt_obj = datetime.datetime.fromtimestamp(time.mktime(entry.published_parsed), UTC_TZ).astimezone(HK_TZ)
                dt_str = dt_obj.strftime('%Y-%m-%d %H:%M')
            
            news_list.append({
                'source': site_name,
                'title': title,
                'link': entry.link,
                'time': dt_str,
                'color': color,
                'method': 'Proxy' 
            })
        return news_list
    except:
        return []

def fetch_rss_or_api(config):
    data = []
    
    try:
        # ç‰¹åˆ¥ç…§æ–™ï¼šNow æ–°è API (å…§éƒ¨ JSON æ¥å£)
        if config['type'] == 'now_api':
             api_url = "https://newsapi1.now.com/pccw-news-api/api/getNewsListv2?category=119&pageNo=1"
             r = requests.get(api_url, headers=HEADERS, timeout=8)
             data_list = r.json()
             
             # è™•ç† JSON çµæ§‹
             items_list = []
             if isinstance(data_list, list):
                 items_list = data_list
             elif isinstance(data_list, dict):
                 # å˜—è©¦å°‹æ‰¾å¸¸è¦‹çš„ key
                 for k in ['data', 'items', 'news']:
                     if k in data_list and isinstance(data_list[k], list):
                         items_list = data_list[k]
                         break
             
             for item in items_list[:10]:
                 title = item.get('newsTitle') or item.get('title')
                 news_id = item.get('newsId')
                 link = f"https://news.now.com/home/local/player?newsId={news_id}" if news_id else ""
                 
                 # è™•ç†æ™‚é–“ (epoch ms)
                 pub_date = item.get('publishDate')
                 dt_str = "æœ€æ–°"
                 if pub_date:
                     try:
                        dt_obj = datetime.datetime.fromtimestamp(pub_date/1000, HK_TZ)
                        dt_str = dt_obj.strftime('%Y-%m-%d %H:%M')
                     except: pass

                 if title and link:
                    data.append({'source': config['name'], 'title': title, 'link': link, 'time': dt_str, 'color': config['color'], 'method': 'API'})

        # é€šç”¨ RSS è™•ç† (å®˜æ–¹ RSS + RSSHub)
        elif config['type'] == 'rss':
            r = requests.get(config['url'], headers=HEADERS, timeout=8)
            feed = feedparser.parse(r.content)
            for entry in feed.entries[:10]:
                dt_str = "æœ€æ–°"
                if hasattr(entry, 'published_parsed') and entry.published_parsed:
                    dt_obj = datetime.datetime.fromtimestamp(time.mktime(entry.published_parsed), UTC_TZ).astimezone(HK_TZ)
                    dt_str = dt_obj.strftime('%Y-%m-%d %H:%M')
                
                title = entry.title
                # è™•ç† Google News æ¨™é¡Œå¾Œç¶´
                if "news.google.com" in config['url']:
                    title = title.rsplit(' - ', 1)[0]

                data.append({'source': config['name'], 'title': title, 'link': entry.link, 'time': dt_str, 'color': config['color'], 'method': 'RSS'})

    except Exception as e:
        print(f"Error fetching {config['name']}: {e}")
        data = []

    # --- Plan B (è‡ªå‹•æ•‘æ´) ---
    if not data and config.get('backup_query'):
        data = fetch_google_proxy(config['backup_query'], config['name'], config['color'])
    
    return data

@st.cache_data(ttl=60)
def get_all_news_data():
    """ å®šç¾©æ‰€æœ‰æ–°èæº """
    
    # æ‚¨çš„ç§äºº RSSHub åœ°å€ (æœ€é‡è¦ï¼)
    RSSHUB_BASE = "https://rsshub-production-9dfc.up.railway.app" 
    
    # ç¦æ¯’æ–°è Google RSS
    ANTIDRUG_RSS = "https://news.google.com/rss/search?q=æ¯’å“+OR+ä¿å®‰å±€+OR+é„§ç‚³å¼·+OR+ç·æ¯’+OR+å¤ªç©ºæ²¹+OR+ä¾è¨—å’ªé…¯+OR+ç¦æ¯’+OR+æ¯’å“æ¡ˆ+OR+æµ·é—œ+OR+ä¿å®‰å±€+OR+é„§ç‚³å¼·+OR+æˆ°æ™‚ç‚¸å½ˆwhen:1d&hl=zh-HK&gl=HK&ceid=HK:zh-Hant"

    configs = [
        # ç¬¬ä¸€è¡Œ (4å€‹)
        {"name": "æ”¿åºœæ–°èï¼ˆä¸­æ–‡ï¼‰", "type": "rss", "url": "https://www.info.gov.hk/gia/rss/general_zh.xml", "color": "#E74C3C", 'backup_query': 'site:info.gov.hk'},
        {"name": "æ”¿åºœæ–°èï¼ˆè‹±æ–‡ï¼‰", "type": "rss", "url": "https://www.info.gov.hk/gia/rss/general_en.xml", "color": "#C0392B", 'backup_query': 'site:info.gov.hk'},
        {"name": "RTHK", "type": "rss", "url": "https://rthk.hk/rthk/news/rss/c_expressnews_clocal.xml", "color": "#FF9800", 'backup_query': 'site:news.rthk.hk'},
        {"name": "Now æ–°èï¼ˆæœ¬åœ°ï¼‰", "type": "now_api", "url": "", "color": "#16A34A", 'backup_query': 'site:news.now.com/home/local'},
        
        # ç¬¬äºŒè¡Œ (4å€‹)
        {"name": "HK01", "type": "rss", "url": f"{RSSHUB_BASE}/hk01/latest", "color": "#2563EB", 'backup_query': 'site:hk01.com'},
        {"name": "on.cc æ±ç¶²", "type": "rss", "url": f"{RSSHUB_BASE}/oncc/zh-hant/news", "color": "#7C3AED", 'backup_query': 'site:hk.on.cc'},
        {"name": "æ˜Ÿå³¶å³æ™‚", "type": "rss", "url": "https://www.stheadline.com/rss", "color": "#F97316", 'backup_query': 'site:stheadline.com'},
        {"name": "æ˜å ±å³æ™‚", "type": "rss", "url": "https://news.mingpao.com/rss/ins/all.xml", "color": "#7C3AED", 'backup_query': 'site:news.mingpao.com'},
        
        # ç¬¬ä¸‰è¡Œ (4å€‹)
        {"name": "i-CABLE æœ‰ç·š", "type": "rss", "url": "https://www.i-cable.com/feed", "color": "#A855F7", 'backup_query': 'site:i-cable.com'},
        {"name": "ç¶“æ¿Ÿæ—¥å ±", "type": "rss", "url": "https://www.hket.com/rss/hongkong", "color": "#7C3AED", 'backup_query': 'site:hket.com'},
        {"name": "ä¿¡å ±å³æ™‚", "type": "rss", "url": f"{RSSHUB_BASE}/hkej/index", "color": "#64748B", 'backup_query': 'site:hkej.com'},
        {"name": "å·´å£«çš„å ±", "type": "rss", "url": "https://www.bastillepost.com/hongkong/feed", "color": "#7C3AED", 'backup_query': 'site:bastillepost.com'},
        
        # é¡å¤–åŠ å…¥ç¦æ¯’æ–°è (ä½œç‚ºç¬¬13å€‹ï¼Œæˆ–æ‚¨å¯ä»¥æ›¿æ›ä¸Šé¢çš„æŸä¸€å€‹)
        # é€™è£¡æˆ‘å…ˆæŠŠå®ƒåŠ åœ¨æœ€å¾Œï¼Œå¦‚æœæ‚¨æƒ³æ”¾ç¬¬ä¸€è¡Œï¼Œè«‹è‡ªè¡Œèª¿æ•´é †åº
        {"name": "ç¦æ¯’/æµ·é—œæ–°è", "type": "rss", "url": ANTIDRUG_RSS, "color": "#D946EF"}, 
    ]

    results_map = {}
    ordered_names = []
    
    for conf in configs:
        items = fetch_rss_or_api(conf)
        results_map[conf['name']] = items
        ordered_names.append(conf)
        
    return results_map, ordered_names

# --- 4. åˆå§‹åŒ– ---

if 'selected_links' not in st.session_state:
    st.session_state.selected_links = set()
if 'generated_text' not in st.session_state:
    st.session_state.generated_text = ""

# æŠ“å–è³‡æ–™
news_data_map, source_configs = get_all_news_data()

# æ‰å¹³åŒ–åˆ—è¡¨ (ä¾›ç”Ÿæˆä½¿ç”¨)
all_flat_news = []
for name, items in news_data_map.items():
    all_flat_news.extend(items)

# --- 5. UI ä½ˆå±€ ---

# Popup Dialog
@st.dialog("ğŸ“„ ç”Ÿæˆçµæœé è¦½")
def show_txt_preview(txt_content):
    st.text_area("å…§å®¹ (å¯å…¨é¸è¤‡è£½)ï¼š", value=txt_content, height=500)
    if st.button("é—œé–‰è¦–çª—"):
        st.rerun()

# å´é‚Šæ¬„
with st.sidebar:
    st.header("âš™ï¸ æ§åˆ¶å°")
    st.caption(f"æ›´æ–°æ™‚é–“: {datetime.datetime.now(HK_TZ).strftime('%H:%M:%S')}")
    
    if st.button("ğŸ”„ ç«‹å³åˆ·æ–°", use_container_width=True):
        st.cache_data.clear()
        st.rerun()
    
    st.divider()
    
    select_count = len(st.session_state.selected_links)
    st.metric("å·²é¸æ–°è", f"{select_count} ç¯‡")
    
    # ç”ŸæˆæŒ‰éˆ•
    if st.button("ğŸ“„ ç”Ÿæˆ TXT å…§å®¹", type="primary", use_container_width=True):
        if select_count == 0:
            st.warning("è«‹å…ˆå‹¾é¸æ–°èï¼")
        else:
            with st.spinner("æ­£åœ¨æå–å…¨æ–‡..."):
                final_txt = ""
                targets = [n for n in all_flat_news if n['link'] in st.session_state.selected_links]
                
                for item in targets:
                    real_link = resolve_google_url(item['link'])
                    content = fetch_full_article(real_link)
                    
                    final_txt += f"{item['source']}ï¼š{item['title']}\n"
                    final_txt += f"[{item['time']}]\n\n"
                    final_txt += f"{content}\n\n"
                    final_txt += f"{real_link}\n\n"
                    final_txt += "Ends\n\n"
                
                # å‘¼å«å½ˆå‡ºè¦–çª—
                show_txt_preview(final_txt)

    # æ¸…ç©ºæŒ‰éˆ•
    if st.button("ğŸ—‘ï¸ ä¸€éµæ¸…ç©ºé¸æ“‡", use_container_width=True):
        st.session_state.selected_links.clear()
        st.rerun()

# ä¸»ç•«é¢
st.title("Tommy Sir å¾Œæ´æœƒä¹‹æ–°èç›£å¯Ÿç³»çµ±")

# æ–°èç¶²æ ¼ (4 æ¬„)
cols_per_row = 4
rows = chunked(source_configs, cols_per_row)

for row in rows:
    cols = st.columns(len(row))
    for col, conf in zip(cols, row):
        with col:
            name = conf['name']
            items = news_data_map.get(name, [])
            
            # æ¨™é¡Œ
            st.markdown(f"""
                <div class='news-source-header' style='border-left: 5px solid {conf['color']}; padding-left: 10px;'>
                    {name}
                    <span class='status-badge'>{len(items)} å‰‡</span>
                </div>
            """, unsafe_allow_html=True)
            
            # åˆ—è¡¨å®¹å™¨ (Clean Style)
            st.markdown('<div class="news-list-container">', unsafe_allow_html=True)
            
            if not items:
                st.markdown('<div style="padding:20px; text-align:center; color:#ccc;">æš«ç„¡è³‡æ–™</div>', unsafe_allow_html=True)
            else:
                for item in items:
                    link = item['link']
                    is_new = is_new_news(item['time'])
                    is_selected = link in st.session_state.selected_links
                    
                    c1, c2 = st.columns([0.15, 0.85])
                    with c1:
                        def update_state(k=link):
                            if k in st.session_state.selected_links:
                                st.session_state.selected_links.remove(k)
                            else:
                                st.session_state.selected_links.add(k)
                        
                        st.checkbox("", key=f"chk_{link}", value=is_selected, on_change=update_state)
                    
                    with c2:
                        new_tag = '<span class="new-badge">NEW!</span>' if is_new else ''
                        text_style = 'class="read-text"' if is_selected else ""
                        
                        st.markdown(f"""
                            <div class="news-item-row">
                                {new_tag}
                                <a href="{link}" target="_blank" {text_style}>
                                    {item['title']}
                                </a><br>
                                <span style="font-size:0.8em; color:#888;">{item['time']}</span>
                            </div>
                        """, unsafe_allow_html=True)
            
            st.markdown('</div>', unsafe_allow_html=True)
