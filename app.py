# app.py
# -*- coding: utf-8 -*-

import datetime
import html
import re
import sys
from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple

import feedparser
import pytz
import requests
import streamlit as st
from bs4 import BeautifulSoup
from dateutil import parser as dtparser
from streamlit_autorefresh import st_autorefresh

# =====================
# åŸºæœ¬è¨­å®š
# =====================
try:
    sys.stdout.reconfigure(encoding="utf-8")
except Exception:
    pass

HK_TZ = pytz.timezone("Asia/Hong_Kong")
NEW_HIGHLIGHT_MINUTES = 20

st.set_page_config(page_title="é¦™æ¸¯æ–°èèšåˆä¸­å¿ƒ", layout="wide", page_icon="ğŸ—ï¸")

# =====================
# CSSï¼ˆä½ è¦çš„ã€Œæ©«å‘ä¸¦åˆ—ã€å¡ç‰‡ + æ–°èæ–°å‡ºç¾ç´…è‰²ï¼‰
# =====================
st.markdown(
    """
<style>
body { font-family: "Microsoft JhengHei","PingFang TC",sans-serif; }

.section-title{
  font-size:1.1rem;font-weight:800;margin:2px 0 8px 0;
}

.card{
  background:#fff;border:1px solid #e5e7eb;border-radius:12px;
  padding:12px;height:520px;display:flex;flex-direction:column;
}

.items{ overflow-y:auto; padding-right:6px; flex:1; }

.item{
  background:#fff;border-left:4px solid #3b82f6;border-radius:8px;
  padding:8px 10px;margin:8px 0;
}

.item.new-item{
  border-left-color:#ef4444 !important;
}

.item a{
  text-decoration:none;color:#111827;font-weight:600;line-height:1.35;
}
.item a:hover{ color:#ef4444; }

.item-meta{
  font-size:0.78rem;color:#6b7280;font-family:monospace;margin-top:2px;
}

.warn{
  font-size:0.82rem;color:#b45309;background:#fffbeb;border:1px solid #fcd34d;
  padding:8px 10px;border-radius:10px;margin:8px 0;
}

.empty{ color:#9ca3af;text-align:center;margin-top:20px; }
</style>
""",
    unsafe_allow_html=True,
)

# =====================
# Model
# =====================
@dataclass
class Article:
    title: str
    link: str
    time_str: str
    color: str
    dt: Optional[datetime.datetime] = None
    # ç”¨ session_state åˆ¤æ–·ã€Œæ–°å‡ºç¾ã€ï¼šç¬¬ä¸€æ¬¡è¦‹åˆ°çš„æ™‚é–“
    first_seen: Optional[datetime.datetime] = None


# =====================
# Helpers
# =====================
def now_hk() -> datetime.datetime:
    return datetime.datetime.now(HK_TZ)


def clean_text(raw: str) -> str:
    raw = html.unescape(raw or "")
    soup = BeautifulSoup(raw, "html.parser")
    return soup.get_text(" ", strip=True)


def _looks_like_html(content: bytes) -> bool:
    head = content[:800].lstrip().lower()
    return head.startswith(b"<!doctype html") or head.startswith(b"<html") or b"<div" in head


def _fetch_bytes(url: str, timeout: int = 12) -> Tuple[Optional[bytes], Optional[str]]:
    """ç”¨ requests æŠ“å…§å®¹ï¼Œé¿å… feedparser ç›´æ¥åƒåˆ° HTMLï¼ˆdiv classï¼‰"""
    try:
        r = requests.get(
            url,
            timeout=timeout,
            headers={
                "User-Agent": "Mozilla/5.0",
                "Accept": "application/rss+xml,application/xml;q=0.9,text/xml;q=0.8,*/*;q=0.7",
            },
        )
        r.raise_for_status()
        content = r.content or b""
        if _looks_like_html(content):
            return None, "å›å‚³çš„æ˜¯ HTMLï¼ˆdiv classï¼‰â€” å¯èƒ½è¢«æ“‹ï¼RSSHub è·¯ç”±å¤±æ•ˆï¼ç«™é»æ”¹ç‰ˆ"
        return content, None
    except Exception as e:
        return None, f"è®€å–å¤±æ•—ï¼š{type(e).__name__}: {e}"


def _safe_get_json(url: str, params: Optional[dict] = None, timeout: int = 12):
    r = requests.get(
        url,
        params=params,
        timeout=timeout,
        headers={"User-Agent": "Mozilla/5.0", "Accept": "application/json,*/*;q=0.8"},
    )
    r.raise_for_status()
    return r.json()


def _epoch_ms_to_dt(ms: int) -> datetime.datetime:
    return datetime.datetime.fromtimestamp(ms / 1000.0, tz=HK_TZ)


def parse_time_from_feed_entry(entry) -> Optional[datetime.datetime]:
    if getattr(entry, "published_parsed", None):
        return datetime.datetime(*entry.published_parsed[:6], tzinfo=pytz.utc).astimezone(HK_TZ)
    if getattr(entry, "updated_parsed", None):
        return datetime.datetime(*entry.updated_parsed[:6], tzinfo=pytz.utc).astimezone(HK_TZ)

    for key in ("published", "updated", "pubDate", "date"):
        val = getattr(entry, key, None)
        if val:
            try:
                dt = dtparser.parse(str(val))
                if dt.tzinfo is None:
                    dt = HK_TZ.localize(dt)
                return dt.astimezone(HK_TZ)
            except Exception:
                pass
    return None


def _ensure_seen_key():
    if "seen_map" not in st.session_state:
        st.session_state["seen_map"] = {}  # type: ignore


def mark_and_flag_new(source_key: str, articles: List[Article]) -> List[Article]:
    """
    - è¨˜éŒ„æ¯æ¢æ–°èé¦–æ¬¡è¦‹åˆ°æ™‚é–“
    - æ–°èã€Œæ–°å‡ºç¾ã€ç¶­æŒ 20 åˆ†é˜ï¼šé¡¯ç¤ºç´…è‰²é‚Šï¼ˆnew-itemï¼‰
    """
    _ensure_seen_key()
    seen_map: Dict[str, str] = st.session_state["seen_map"]  # type: ignore
    now = now_hk()

    for a in articles:
        k = f"{source_key}||{a.link}"
        if k not in seen_map:
            seen_map[k] = now.isoformat()
            a.first_seen = now
        else:
            try:
                a.first_seen = dtparser.parse(seen_map[k]).astimezone(HK_TZ)
            except Exception:
                a.first_seen = now

    return articles


def sort_latest_first(articles: List[Article]) -> List[Article]:
    """
    å…ˆæŒ‰ dtï¼ˆæœ‰å°±ç”¨ï¼‰ï¼Œç„¡ dt å°±ç”¨ first_seenï¼Œå†ç„¡å°±æ”¾å¾Œé¢ã€‚
    """
    def key(a: Article):
        if a.dt:
            return a.dt
        if a.first_seen:
            return a.first_seen
        return datetime.datetime(1970, 1, 1, tzinfo=HK_TZ)

    return sorted(articles, key=key, reverse=True)


def is_new(a: Article) -> bool:
    if not a.first_seen:
        return False
    return (now_hk() - a.first_seen) <= datetime.timedelta(minutes=NEW_HIGHLIGHT_MINUTES)


# =====================
# Fetchers
# =====================
@st.cache_data(ttl=60)
def fetch_rss_today(url: str, color: str, limit: int = 10) -> Tuple[List[Article], Optional[str]]:
    content, warn = _fetch_bytes(url, timeout=12)
    if warn:
        return [], warn

    feed = feedparser.parse(content)
    today = now_hk().date()

    out_today: List[Article] = []
    out_latest: List[Article] = []

    for e in feed.entries or []:
        title = clean_text(getattr(e, "title", ""))
        link = getattr(e, "link", "")
        if not title or not link:
            continue

        dt = parse_time_from_feed_entry(e)
        if dt:
            art = Article(title=title, link=link, time_str=dt.strftime("%H:%M"), color=color, dt=dt)
            out_latest.append(art)
            if dt.date() == today:
                out_today.append(art)
        else:
            out_latest.append(Article(title=title, link=link, time_str="ä»Šæ—¥", color=color, dt=None))

        if len(out_latest) >= limit:
            break

    if out_today:
        return out_today[:limit], None

    if out_latest:
        return out_latest[:limit], "æ­¤ä¾†æºæœªæä¾›å¯è§£ææ™‚é–“ï¼æ—¥æœŸï¼Œå·²æ”¹ç‚ºé¡¯ç¤ºæœ€æ–° 10 æ¢"

    return [], "RSS ç„¡å…§å®¹æˆ–æš«æ™‚è®€å–ä¸åˆ°"


@st.cache_data(ttl=60)
def fetch_rss_latest(url: str, color: str, limit: int = 10) -> Tuple[List[Article], Optional[str]]:
    content, warn = _fetch_bytes(url, timeout=12)
    if warn:
        return [], warn

    feed = feedparser.parse(content)
    out: List[Article] = []

    for e in feed.entries or []:
        title = clean_text(getattr(e, "title", ""))
        link = getattr(e, "link", "")
        if not title or not link:
            continue

        dt = parse_time_from_feed_entry(e)
        time_str = dt.strftime("%H:%M") if dt else "å³æ™‚"
        out.append(Article(title=title, link=link, time_str=time_str, color=color, dt=dt))

        if len(out) >= limit:
            break

    if out:
        return out[:limit], None

    return [], "RSS ç„¡å…§å®¹æˆ–æš«æ™‚è®€å–ä¸åˆ°"


@st.cache_data(ttl=60)
def fetch_now_local_today(color: str, limit: int = 10) -> Tuple[List[Article], Optional[str]]:
    """
    Now æ–°èï¼ˆæœ¬åœ°ï¼‰ï¼š
    - ç”¨ä½ ç¢ºèªå¯ç”¨çš„ APIï¼šgetNewsListv2?category=119&pageNo=1
    - å³ä½¿ webUrl/shareUrl ç‚º nullï¼Œä»ç”¨ newsId è‡ªå‹•ç Œå›å¯æ‰“é–‹çš„ player link
    """
    today = now_hk().date()
    NOW_API = "https://newsapi1.now.com/pccw-news-api/api/getNewsListv2"

    out_today: List[Article] = []
    out_latest: List[Article] = []

    try:
        data = _safe_get_json(NOW_API, {"category": 119, "pageNo": 1}, timeout=12)

        # å– list
        candidates = None
        if isinstance(data, list):
            candidates = data
        elif isinstance(data, dict):
            for k in ("data", "list", "news", "items", "result"):
                v = data.get(k)
                if isinstance(v, list):
                    candidates = v
                    break
            if candidates is None:
                for v in data.values():
                    if isinstance(v, dict):
                        for kk in ("data", "list", "news", "items", "result"):
                            vv = v.get(kk)
                            if isinstance(vv, list):
                                candidates = vv
                                break
                    if candidates is not None:
                        break

        if not candidates:
            return [], "Now API å›å‚³çµæ§‹å·²è®Šï¼ˆæ‰¾ä¸åˆ°æ–°èåˆ—è¡¨ï¼‰"

        for it in candidates:
            if not isinstance(it, dict):
                continue

            title = clean_text(str(it.get("title") or it.get("newsTitle") or it.get("headline") or ""))
            news_id = it.get("newsId")

            link = str(it.get("webUrl") or it.get("shareUrl") or it.get("url") or it.get("link") or "")
            if link.startswith("/"):
                link = "https://news.now.com" + link

            # webUrl ä¿‚ null æ™‚ï¼Œç”¨ newsId ç Œ player URL
            if (not link) and news_id:
                link = f"https://news.now.com/home/local/player?newsId={news_id}"

            # æ™‚é–“ï¼špublishDate epoch ms
            dt = None
            time_str = "ä»Šæ—¥"
            raw = it.get("publishDate") or it.get("publishTime") or it.get("publishedAt") or it.get("date")
            if raw is not None:
                try:
                    if isinstance(raw, (int, float)) or (isinstance(raw, str) and raw.isdigit()):
                        dt = _epoch_ms_to_dt(int(raw))
                    else:
                        dt = dtparser.parse(str(raw))
                        if dt.tzinfo is None:
                            dt = HK_TZ.localize(dt)
                        dt = dt.astimezone(HK_TZ)
                    time_str = dt.strftime("%H:%M")
                except Exception:
                    dt = None
                    time_str = "ä»Šæ—¥"

            if title and link:
                art = Article(title=title, link=link, time_str=time_str, color=color, dt=dt)
                out_latest.append(art)
                if dt and dt.date() == today:
                    out_today.append(art)

            if len(out_latest) >= limit:
                break

        if out_today:
            return out_today[:limit], None

        if out_latest:
            return out_latest[:limit], "æœªèƒ½ç¯©å‡ºã€ä»Šæ—¥ã€æ–°èï¼Œå·²æ”¹ç‚ºé¡¯ç¤ºæœ€æ–° 10 æ¢"

        return [], "Now API æœ‰å›å‚³ä½†æœªèƒ½è§£æåˆ°æœ‰æ•ˆæ–°èï¼ˆå¯èƒ½ç¼ºå°‘ title/link/newsIdï¼‰"

    except Exception as e:
        return [], f"Now API è®€å–å¤±æ•—ï¼š{type(e).__name__}: {e}"


# =====================
# Render
# =====================
def build_card_html(title: str, articles: List[Article], warn: Optional[str] = None) -> str:
    warn_html = f"<div class='warn'>âš ï¸ {html.escape(warn)}</div>" if warn else ""

    if not articles:
        items_html = "<div class='empty'>ä»Šæ—¥æš«ç„¡æ–°è</div>"
    else:
        parts = []
        for a in articles:
            new_cls = "new-item" if is_new(a) else ""
            parts.append(
                f"""
                <div class="item {new_cls}" style="border-left-color:{a.color}">
                  <a href="{html.escape(a.link)}" target="_blank" rel="noopener noreferrer">{html.escape(a.title)}</a>
                  <div class="item-meta">ğŸ• {html.escape(a.time_str)}</div>
                </div>
                """
            )
        items_html = "".join(parts)

    return f"""
    <div class="section-title">{html.escape(title)}</div>
    <div class="card">
      {warn_html}
      <div class="items">
        {items_html}
      </div>
    </div>
    """


def render_source(
    col,
    source_key: str,
    title: str,
    fetch_fn,
    *fetch_args,
    limit: int = 10,
):
    with col:
        arts, warn = fetch_fn(*fetch_args, limit)
        # è¨˜éŒ„é¦–æ¬¡è¦‹åˆ°æ™‚é–“ï¼Œåšã€Œæ–°å‡ºç¾ã€ç´…è‰² 20 åˆ†é˜
        arts = mark_and_flag_new(source_key, arts)
        # å…¨éƒ¨æŒ‰æ™‚é–“ç”±æ–°åˆ°èˆŠ
        arts = sort_latest_first(arts)
        st.markdown(build_card_html(title, arts, warn), unsafe_allow_html=True)


# =====================
# URLsï¼ˆä½ æä¾›çš„ä¾†æº + RSSHubï¼‰
# =====================
GOV_ZH = "https://www.info.gov.hk/gia/rss/general_zh.xml"
GOV_EN = "https://www.info.gov.hk/gia/rss/general_en.xml"
RTHK = "https://rthk.hk/rthk/news/rss/c_expressnews_clocal.xml"

# ä½  RSSHub åŸŸåï¼ˆå¯åœ¨ sidebar æ”¹ï¼‰
DEFAULT_RSSHUB = "https://rsshub-production-9dfc.up.railway.app"

# =====================
# UI
# =====================
st.title("ğŸ—ï¸ é¦™æ¸¯æ–°èèšåˆä¸­å¿ƒ")
st.caption(f"æœ€å¾Œæ›´æ–°ï¼ˆé¦™æ¸¯æ™‚é–“ï¼‰ï¼š{now_hk().strftime('%Y-%m-%d %H:%M:%S')}")

with st.sidebar:
    st.subheader("è¨­å®š")
    rsshub_base = st.text_input("RSSHub Base URL", value=DEFAULT_RSSHUB).strip().rstrip("/")
    limit = st.slider("æ¯å€‹åª’é«”é¡¯ç¤ºæ¢æ•¸", 5, 30, 10, 1)
    if st.toggle("æ¯åˆ†é˜è‡ªå‹•æ›´æ–°", value=True):
        st_autorefresh(interval=60_000, key="auto")

# RSSHub è·¯ç”±ï¼ˆæŒ‰ä½ çµ¦çš„æ¸…å–®ï¼‰
HK01 = f"{rsshub_base}/hk01/latest"
ONCC = f"{rsshub_base}/oncc/zh-hant/news"
TVB = f"{rsshub_base}/tvb/news/tc"
HKEJ = f"{rsshub_base}/hkej/index"
STHEADLINE = f"{rsshub_base}/stheadline/std/realtime"
ICABLE = f"{rsshub_base}/icable/all"

# æ³¨æ„ï¼šä½ è©± RSSHub Now å£å’—ï¼Œæ‰€ä»¥ Now æ”¹ç”¨ APIï¼ˆå””å†ç”¨ rsshub now/newsï¼‰
# NOWï¼ˆæœ¬åœ°ï¼‰ç”¨ fetch_now_local_today()

# =====================
# ç‰ˆé¢ï¼ˆä¿æŒã€Œæ¯å€‹å¹³å°æ©«å‘ä¸¦åˆ—ã€ï¼Œä¸æ··åˆï¼‰
# ä½ å¯ä»¥æŒ‰è‡ªå·±åœ–äºŒçš„æ’åˆ—ï¼Œæ”¹ä¸‹é¢ row çš„é †åºï¼Œä½†æ¯æ ¼éƒ½ä¿‚ç¨ç«‹å¹³å°
# =====================

# Row 1
row1 = st.columns(4)
render_source(row1[0], "gov_zh", "æ”¿åºœæ–°èï¼ˆä¸­æ–‡ï¼‰", fetch_rss_today, GOV_ZH, "#E74C3C", limit=limit)
render_source(row1[1], "gov_en", "æ”¿åºœæ–°èï¼ˆè‹±æ–‡ï¼‰", fetch_rss_today, GOV_EN, "#C0392B", limit=limit)
render_source(row1[2], "rthk", "RTHK", fetch_rss_today, RTHK, "#FF9800", limit=limit)
render_source(row1[3], "now_local", "Nowï¼ˆæœ¬åœ° / æ¸¯èï¼‰", fetch_now_local_today, "#2563EB", limit=limit)

# Row 2
row2 = st.columns(4)
render_source(row2[0], "hk01", "HK01", fetch_rss_latest, HK01, "#0ea5e9", limit=limit)
render_source(row2[1], "oncc", "on.cc æ±ç¶²", fetch_rss_latest, ONCC, "#111827", limit=limit)
render_source(row2[2], "tvb", "TVB æ–°è", fetch_rss_latest, TVB, "#16a34a", limit=limit)
render_source(row2[3], "hkej", "ä¿¡å ±å³æ™‚", fetch_rss_latest, HKEJ, "#7c3aed", limit=limit)

# Row 3
row3 = st.columns(4)
render_source(row3[0], "stheadline", "æ˜Ÿå³¶å³æ™‚", fetch_rss_latest, STHEADLINE, "#f97316", limit=limit)
render_source(row3[1], "icable", "i-CABLE æœ‰ç·š", fetch_rss_latest, ICABLE, "#dc2626", limit=limit)
# ä½ ä¹‹å¾Œæƒ³åŠ åª’é«”å°±åŠ åœ¨é€™å…©æ ¼ï¼ˆæš«ç•™ç©ºï¼‰
with row3[2]:
    st.markdown(build_card_html("ï¼ˆé ç•™ï¼‰", [], "ä½ å¯ä»¥åœ¨é€™æ ¼åŠ ä¸‹ä¸€å€‹ RSSHub/å®˜æ–¹ RSS"), unsafe_allow_html=True)
with row3[3]:
    st.markdown(build_card_html("ï¼ˆé ç•™ï¼‰", [], "ä½ å¯ä»¥åœ¨é€™æ ¼åŠ ä¸‹ä¸€å€‹ RSSHub/å®˜æ–¹ RSS"), unsafe_allow_html=True)
