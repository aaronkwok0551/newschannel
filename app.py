# -*- coding: utf-8 -*-
import streamlit as st
import requests
import feedparser
import datetime
import pytz
import urllib.parse
import time
from bs4 import BeautifulSoup
import sys
from streamlit_autorefresh import st_autorefresh

# è¨­å®šé è¨­ç·¨ç¢¼
try:
    sys.stdout.reconfigure(encoding='utf-8')
except:
    pass

# --- 1. é é¢èˆ‡è‡ªå®šç¾©æ¨£å¼ ---
st.set_page_config(
    page_title="Tommy Sir å¾Œæ´æœƒä¹‹æ–°èç›£å¯Ÿç³»çµ±",
    page_icon="ğŸ“°",
    layout="wide"
)

# è‡ªå‹•åˆ·æ–° (æ¯ 60 ç§’)
st_autorefresh(interval=60 * 1000, limit=None, key="news_autoupdate")

st.markdown("""
<style>
    /* é–ƒçˆç‰¹æ•ˆ */
    @keyframes blinker { 50% { opacity: 0; } }
    .new-badge {
        color: #ff4b4b;
        font-weight: bold;
        animation: blinker 1s linear infinite;
        margin-right: 5px;
        font-size: 0.8em;
    }
    .read-text {
        color: #a0a0a0 !important;
        text-decoration: none;
        font-weight: normal !important;
    }
    .stCheckbox { margin-bottom: 0px; }
    .news-source-header { 
        font-size: 1.2em; 
        font-weight: bold; 
        color: #1e293b; 
        margin-top: 5px; 
        margin-bottom: 15px;
        padding-bottom: 5px;
        border-bottom: 2px solid #ddd;
    }
    a { text-decoration: none; color: #2980b9; transition: 0.3s; }
    a:hover { color: #e74c3c; }
    
    /* èª¿æ•´åˆ—çš„å°é½Š */
    div[data-testid="column"] {
        background-color: #ffffff;
        padding: 10px;
        border-radius: 5px;
    }
</style>
""", unsafe_allow_html=True)

# è¨­å®šæ™‚å€
HK_TZ = pytz.timezone('Asia/Hong_Kong')
UTC_TZ = pytz.timezone('UTC')

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7',
}

# --- 2. æ ¸å¿ƒåŠŸèƒ½å‡½å¼ (é‡å°ç‰¹å®šç¶²ç«™å„ªåŒ–) ---

def fetch_full_article(url):
    """ é‡å°ä¸åŒå¹³å°å„ªåŒ–æŠ“å–é‚è¼¯ """
    # é è™•ç† Google News é€£çµ
    if "news.google.com" in url:
        url = resolve_google_url(url)

    try:
        r = requests.get(url, headers=HEADERS, timeout=10)
        r.encoding = 'utf-8' # å¤§éƒ¨åˆ†æ¸¯åª’æ˜¯ utf-8
        soup = BeautifulSoup(r.text, 'html.parser')
        
        # ç§»é™¤å¹²æ“¾å…ƒç´ 
        for tag in soup(['script', 'style', 'header', 'footer', 'nav', 'iframe', 'noscript', 'meta']):
            tag.decompose()

        paragraphs = []

        # --- é‡å°ä¸åŒç¶²ç«™çš„ç‰¹å®šè§£æé‚è¼¯ ---
        
        # 1. ç„¡ç·šæ–°è (TVB)
        if "news.tvb.com" in url:
            # TVB é€šå¸¸åœ¨ div class="content-node-details" æˆ– generic content ä¸­
            content_div = soup.find('div', class_='content-node-details')
            if not content_div:
                content_div = soup.find('div', class_='desc')
            if content_div:
                paragraphs = content_div.find_all(['p', 'div'], recursive=False)

        # 2. Now æ–°è
        elif "news.now.com" in url:
            # Now æ–°èé€šå¸¸çµæ§‹ï¼š .newsLeading (å°èª) + .newsContent (å…§æ–‡)
            leading = soup.find('div', class_='newsLeading')
            content = soup.find('div', class_='newsContent')
            if leading: paragraphs.append(leading)
            if content: paragraphs.append(content)

        # 3. å•†æ¥­é›»å° (881903)
        elif "881903.com" in url:
            # å•†å°çµæ§‹è¼ƒäº‚ï¼Œé€šå¸¸åœ¨ div.news-content
            content_div = soup.find('div', class_='news-content')
            if content_div:
                paragraphs = content_div.find_all('p')

        # 4. é¦™æ¸¯ 01
        elif "hk01.com" in url:
            content_div = soup.find('article')
            if content_div:
                paragraphs = content_div.find_all('p')

        # 5. é€šç”¨å¾Œå‚™æ–¹æ¡ˆ (Fallback)
        if not paragraphs:
            # å°‹æ‰¾å«æœ‰å¤§é‡æ–‡å­—çš„ div
            main_div = soup.find('div', class_=lambda x: x and ('article' in x.lower() or 'content' in x.lower()))
            if main_div:
                paragraphs = main_div.find_all('p')
            else:
                paragraphs = soup.find_all('p')

        # æå–æ–‡å­—
        full_text_list = []
        for p in paragraphs:
            text = p.get_text().strip()
            if len(text) > 5 and "è«‹æŒ‰æ­¤" not in text and "åŸæ–‡ç¶²å€" not in text:
                full_text_list.append(text)
        
        full_text = "\n\n".join(full_text_list)
        
        if len(full_text) < 30:
            return f"å…§å®¹æŠ“å–éçŸ­ï¼Œè«‹ç›´æ¥æŸ¥çœ‹ç¶²é ï¼š{url}"
        
        return full_text

    except Exception as e:
        return f"æŠ“å–å¤±æ•— ({str(e)}) - è«‹æ‰‹å‹•æŸ¥çœ‹: {url}"

def resolve_google_url(url):
    """ è§£æ Google Redirect URL """
    try:
        # allow_redirects=True æœƒè‡ªå‹•è·³è½‰åˆ°æœ€çµ‚ç¶²å€
        r = requests.get(url, headers=HEADERS, timeout=5, stream=True)
        return r.url
    except:
        return url

def is_new_news(published_time_str):
    try:
        pub_time = datetime.datetime.strptime(published_time_str, '%Y-%m-%d %H:%M')
        pub_time = HK_TZ.localize(pub_time)
        now = datetime.datetime.now(HK_TZ)
        diff = (now - pub_time).total_seconds() / 60
        return 0 <= diff <= 30 # æ”¾å¯¬åˆ° 30 åˆ†é˜å…§
    except:
        return False

# å¿«å–æ•¸æ“šæŠ“å–
@st.cache_data(ttl=60)
def fetch_news_data(func_name, *args):
    if func_name == "fetch_hk01":
        return fetch_hk01()
    elif func_name == "fetch_google_rss":
        return fetch_google_rss(*args)
    elif func_name == "fetch_direct_rss":
        return fetch_direct_rss(*args)
    return []

def fetch_google_rss(site_domain, site_name, color):
    query = urllib.parse.quote(f"site:{site_domain}")
    rss_url = f"https://news.google.com/rss/search?q={query}+when:1d&hl=zh-HK&gl=HK&ceid=HK:zh-Hant"
    feed = feedparser.parse(rss_url)
    news = []
    for entry in feed.entries[:8]:
        title = entry.title.rsplit(" - ", 1)[0]
        dt_str = ""
        if hasattr(entry, 'published_parsed'):
            dt_obj = datetime.datetime.fromtimestamp(time.mktime(entry.published_parsed), UTC_TZ).astimezone(HK_TZ)
            dt_str = dt_obj.strftime('%Y-%m-%d %H:%M')
        news.append({'source': site_name, 'title': title, 'link': entry.link, 'time': dt_str, 'color': color})
    return news

def fetch_direct_rss(url, name, color):
    try:
        r = requests.get(url, headers=HEADERS, timeout=8)
        feed = feedparser.parse(r.content)
        news = []
        for entry in feed.entries[:8]:
            dt_str = ""
            if hasattr(entry, 'published_parsed'):
                dt_obj = datetime.datetime.fromtimestamp(time.mktime(entry.published_parsed), UTC_TZ).astimezone(HK_TZ)
                dt_str = dt_obj.strftime('%Y-%m-%d %H:%M')
            link = entry.link
            # Now æ–°èçš„ RSS é€£çµæœ‰æ™‚éœ€è¦è£œå…¨
            if 'news.now.com' in url and 'http' not in link:
                link = f"https://news.now.com{link}"
            
            news.append({'source': name, 'title': entry.title, 'link': link, 'time': dt_str, 'color': color})
        return news
    except: return []

def fetch_hk01():
    try:
        r = requests.get("https://web-data.api.hk01.com/v2/feed/category/0", headers=HEADERS, timeout=5)
        items = r.json().get('items', [])[:8]
        news = []
        for item in items:
            raw = item.get('data', {})
            dt_str = datetime.datetime.fromtimestamp(raw.get('publishTime'), HK_TZ).strftime('%Y-%m-%d %H:%M')
            news.append({'source': "HK01", 'title': raw.get('title'), 'link': raw.get('publishUrl'), 'time': dt_str, 'color': "#184587"})
        return news
    except: return []

# --- 3. åˆå§‹åŒ–ç‹€æ…‹ ---

if 'selected_links' not in st.session_state:
    st.session_state.selected_links = set()
if 'generated_text' not in st.session_state:
    st.session_state.generated_text = ""
if 'all_current_news' not in st.session_state:
    st.session_state.all_current_news = []

# --- 4. å´é‚Šæ¬„æ§åˆ¶èˆ‡é¡¯ç¤º ---

with st.sidebar:
    st.title("âš™ï¸ æ§åˆ¶å°")
    
    # æŒ‰éˆ•å€ (æ”¾åœ¨æœ€ä¸Šæ–¹)
    col_btn1, col_btn2 = st.columns(2)
    
    with col_btn1:
        if st.button("ğŸ”„ åˆ·æ–°æ–°è"):
            st.cache_data.clear()
            st.session_state.all_current_news = []
            st.rerun()

    with col_btn2:
        if st.button("ğŸ—‘ï¸ æ¸…ç©ºé¸æ“‡"):
            st.session_state.selected_links = set() # ç›´æ¥é‡ç½® Set
            st.session_state.generated_text = ""
            st.rerun()

    st.divider()
    
    # çµ±è¨ˆ
    st.write(f"å·²é¸æ–°è: **{len(st.session_state.selected_links)}** ç¯‡")

    # ç”ŸæˆæŒ‰éˆ•
    if st.button("ğŸ“„ ç”Ÿæˆ TXT å…§å®¹", type="primary", use_container_width=True):
        if not st.session_state.selected_links:
            st.warning("è«‹å…ˆåœ¨å³å´å‹¾é¸æ–°èï¼")
        else:
            with st.spinner("æ­£åœ¨å‰å¾€å„å¤§ç¶²ç«™æŠ“å–å…§æ–‡..."):
                final_txt = ""
                # å¾ç·©å­˜çš„æ–°èåˆ—è¡¨ä¸­éæ¿¾
                selected_items = [
                    item for item in st.session_state.all_current_news 
                    if item['link'] in st.session_state.selected_links
                ]
                
                count = 1
                total = len(selected_items)
                progress_bar = st.progress(0)

                for idx, item in enumerate(selected_items):
                    content = fetch_full_article(item['link'])
                    final_txt += f"ã€æ–°è {idx+1}ã€‘{item['source']}ï¼š{item['title']}\n"
                    final_txt += f"ç™¼å¸ƒæ™‚é–“ï¼š{item['time']}\n"
                    final_txt += "-" * 20 + "\n"
                    final_txt += f"{content}\n\n"
                    final_txt += f"é€£çµï¼š{item['link']}\n"
                    final_txt += "Ends\n\n" + "="*30 + "\n\n"
                    progress_bar.progress((idx + 1) / total)

                st.session_state.generated_text = final_txt
                progress_bar.empty()

    # --- ç”Ÿæˆçµæœé¡¯ç¤ºå€ (ç§»è‡³ Sidebar) ---
    if st.session_state.generated_text:
        st.markdown("---")
        st.success("âœ… ç”Ÿæˆå®Œæˆï¼")
        st.text_area("TXT å…§å®¹é è¦½ (Ctrl+A å…¨é¸è¤‡è£½)", 
                     value=st.session_state.generated_text, 
                     height=600)

# --- 5. ä¸»ä»‹é¢ï¼šæ–°èé¡¯ç¤ºå€ (å°é½Šå„ªåŒ–ç‰ˆ) ---

st.title("Tommy Sir å¾Œæ´æœƒä¹‹æ–°èç›£å¯Ÿç³»çµ±")
st.caption(f"æœ€å¾ŒåŒæ­¥æ™‚é–“: {datetime.datetime.now(HK_TZ).strftime('%Y-%m-%d %H:%M:%S')} (æ¯ 60 ç§’è‡ªå‹•æ›´æ–°)")

# å®šç¾©æ–°èæºé…ç½®
# æ³¨æ„ï¼šNow æ–°èæ”¹ç”¨ fetch_direct_rss å˜—è©¦æ›´ç©©å®šæŠ“å–ï¼Œå¦‚æœ RSS å¤±æ•—æœƒè‡ªå‹•é€€å›
sources_config = [
    # ç¬¬ä¸€æ¬„
    [
        ("HK01", "fetch_hk01", []),
        ("é¦™æ¸¯é›»å°", "fetch_direct_rss", ["https://rthk.hk/rthk/news/rss/c_expressnews_clocal.xml", "é¦™æ¸¯é›»å°", "#FF9800"]),
    ],
    # ç¬¬äºŒæ¬„
    [
        ("ç„¡ç·šæ–°è (TVB)", "fetch_google_rss", ["news.tvb.com/tc/local", "ç„¡ç·šæ–°è", "#27ae60"]),
        ("æœ‰ç·šæ–°è", "fetch_direct_rss", ["https://www.i-cable.com/feed/", "æœ‰ç·šæ–°è", "#c0392b"]),
    ],
    # ç¬¬ä¸‰æ¬„
    [
        ("Now æ–°è", "fetch_google_rss", ["news.now.com/home/local", "Now æ–°è", "#E65100"]),
        ("å•†æ¥­é›»å°", "fetch_google_rss", ["881903.com", "å•†å°", "#F1C40F"]),
    ]
]

# å‰µå»ºå›ºå®šçš„ä¸‰æ¬„ä½ˆå±€ (è§£æ±ºé‹¸é½’å•é¡Œ)
cols = st.columns(3)
temp_all_news = []

# éæ­·ä¸‰æ¬„é…ç½®
for col_idx, column_sources in enumerate(sources_config):
    with cols[col_idx]:
        for name, func_name, args in column_sources:
            # é¡¯ç¤ºä¾†æºæ¨™é¡Œ
            st.markdown(f"<div class='news-source-header'>{name}</div>", unsafe_allow_html=True)
            
            # æŠ“å–æ•¸æ“š
            news_items = fetch_news_data(func_name, *args)
            temp_all_news.extend(news_items)
            
            if not news_items:
                st.info("æš«ç„¡æ›´æ–°")
            else:
                for item in news_items:
                    link = item['link']
                    is_new = is_new_news(item['time'])
                    
                    # ç¢ºä¿æŒ‰éˆ•ç‹€æ…‹æ­£ç¢º
                    is_selected = link in st.session_state.selected_links
                    
                    # ä½ˆå±€ï¼šCheckbox + æ¨™é¡Œ
                    c1, c2 = st.columns([0.1, 0.9])
                    with c1:
                        # é€™è£¡çš„é—œéµæ˜¯ key å¿…é ˆå”¯ä¸€ï¼Œä¸”ç‹€æ…‹è¦è·Ÿ session_state åŒæ­¥
                        checked = st.checkbox(
                            "", 
                            key=f"chk_{link}", 
                            value=is_selected
                        )
                        # æ›´æ–°ç‹€æ…‹é‚è¼¯
                        if checked:
                            st.session_state.selected_links.add(link)
                        else:
                            st.session_state.selected_links.discard(link)
                            
                    with c2:
                        new_tag = '<span class="new-badge">NEW!</span>' if is_new else ''
                        # æ ¹æ“šæ˜¯å¦é¸ä¸­æ”¹è®Šæ–‡å­—æ¨£å¼
                        text_style = 'class="read-text"' if is_selected else ""
                        
                        st.markdown(f"""
                            <div style="line-height:1.4; margin-bottom:10px;">
                                {new_tag}
                                <a href="{link}" target="_blank" {text_style}>
                                    {item['title']}
                                </a>
                                <br>
                                <span style="font-size:0.8em; color:#888;">{item['time']}</span>
                            </div>
                        """, unsafe_allow_html=True)

# æ›´æ–°æ‰€æœ‰æ–°èç·©å­˜
st.session_state.all_current_news = temp_all_news
