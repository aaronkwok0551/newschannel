# -*- coding: utf-8 -*-
import streamlit as st
import requests
import feedparser
import datetime
import pytz
import urllib.parse
import time
from bs4 import BeautifulSoup
import sys
from streamlit_autorefresh import st_autorefresh

# è¨­å®šé è¨­ç·¨ç¢¼
try:
    sys.stdout.reconfigure(encoding='utf-8')
except:
    pass

# --- 1. é é¢èˆ‡è‡ªå®šç¾©æ¨£å¼ ---
st.set_page_config(
    page_title="Tommy Sir å¾Œæ´æœƒä¹‹æ–°èç›£å¯Ÿç³»çµ±",
    page_icon="ğŸ“°",
    layout="wide"
)

# è‡ªå‹•åˆ·æ–° (æ¯ 60 ç§’)
st_autorefresh(interval=60 * 1000, limit=None, key="news_autoupdate")

st.markdown("""
<style>
    /* é–ƒçˆç‰¹æ•ˆ */
    @keyframes blinker { 50% { opacity: 0; } }
    .new-badge {
        color: #ff4b4b;
        font-weight: bold;
        animation: blinker 1s linear infinite;
        margin-right: 5px;
        font-size: 0.8em;
    }
    .read-text {
        color: #a0a0a0 !important;
        text-decoration: none;
        font-weight: normal !important;
    }
    .stCheckbox { margin-bottom: 0px; }
    .news-source-header { 
        font-size: 1.2em; 
        font-weight: bold; 
        color: #1e293b; 
        margin-top: 5px; 
        margin-bottom: 15px;
        padding-bottom: 5px;
        border-bottom: 2px solid #ddd;
    }
    a { text-decoration: none; color: #2980b9; transition: 0.3s; }
    a:hover { color: #e74c3c; }
    
    /* èª¿æ•´åˆ—çš„å°é½Š */
    div[data-testid="column"] {
        background-color: #ffffff;
        padding: 10px;
        border-radius: 5px;
    }
    /* ä¿®æ­£ HTML åŸå§‹ç¢¼å¤–æ´©å•é¡Œï¼Œå¼·åˆ¶éš±è—å¯èƒ½çš„ Raw Code */
    code { display: none; }
</style>
""", unsafe_allow_html=True)

# è¨­å®šæ™‚å€
HK_TZ = pytz.timezone('Asia/Hong_Kong')
UTC_TZ = pytz.timezone('UTC')

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
}

# --- 2. æ ¸å¿ƒåŠŸèƒ½å‡½å¼ ---

def get_real_url(url):
    """
    å˜—è©¦è§£æ Google News çš„é‡å®šå‘ç¶²å€ï¼Œé‚„åŸæˆåŸå§‹ç¶²å€ã€‚
    ä¸»è¦ç”¨æ–¼ç”Ÿæˆ TXT æ™‚ï¼Œå› ç‚ºé€™éœ€è¦ç¶²è·¯è«‹æ±‚ï¼Œä¸å»ºè­°åœ¨åˆ—è¡¨é¡¯ç¤ºæ™‚å°æ¯æ¢éƒ½åšã€‚
    """
    if "news.google.com" not in url:
        return url
    try:
        # allow_redirects=True æœƒè‡ªå‹•è·Ÿéš¨è·³è½‰ç›´åˆ°æœ€å¾Œçš„çœŸå¯¦ç¶²å€
        r = requests.head(url, headers=HEADERS, allow_redirects=True, timeout=5)
        return r.url
    except:
        # å¦‚æœ HEAD è«‹æ±‚å¤±æ•—ï¼Œå˜—è©¦ GET
        try:
            r = requests.get(url, headers=HEADERS, timeout=5)
            return r.url
        except:
            return url

def fetch_full_article(url):
    """ æŠ“å–å…§æ–‡ï¼Œä¸¦åœ¨æ­¤è™•ç¢ºä¿ç¶²å€æ˜¯çœŸå¯¦ç¶²å€ """
    
    # 1. å…ˆæŠŠç¶²å€é‚„åŸæˆçœŸå¯¦ç¶²å€ (é‡å° Google é€£çµ)
    real_url = get_real_url(url)
    
    try:
        r = requests.get(real_url, headers=HEADERS, timeout=10)
        r.encoding = 'utf-8'
        soup = BeautifulSoup(r.text, 'html.parser')
        
        # ç§»é™¤å¹²æ“¾å…ƒç´ 
        for tag in soup(['script', 'style', 'header', 'footer', 'nav', 'iframe', 'noscript', 'meta', 'svg', 'button']):
            tag.decompose()

        paragraphs = []

        # --- é‡å°ä¸åŒç¶²ç«™çš„ç‰¹å®šè§£æé‚è¼¯ ---
        
        # 1. ç„¡ç·šæ–°è (TVB)
        if "news.tvb.com" in real_url:
            content_div = soup.find('div', class_='content-node-details') or soup.find('div', class_='desc')
            if content_div: paragraphs = content_div.find_all(['p', 'div'], recursive=False)

        # 2. Now æ–°è
        elif "news.now.com" in real_url:
            leading = soup.find('div', class_='newsLeading')
            content = soup.find('div', class_='newsContent')
            if leading: paragraphs.append(leading)
            if content: paragraphs.append(content)

        # 3. å•†æ¥­é›»å° (881903)
        elif "881903.com" in real_url:
            content_div = soup.find('div', class_='news-content')
            if content_div: paragraphs = content_div.find_all('p')

        # 4. é¦™æ¸¯ 01
        elif "hk01.com" in real_url:
            content_div = soup.find('article')
            if content_div: paragraphs = content_div.find_all('p')

        # 5. é€šç”¨å¾Œå‚™æ–¹æ¡ˆ
        if not paragraphs:
            main_div = soup.find('div', class_=lambda x: x and ('article' in x.lower() or 'content' in x.lower()))
            if main_div:
                paragraphs = main_div.find_all('p')
            else:
                paragraphs = soup.find_all('p')

        full_text_list = []
        for p in paragraphs:
            text = p.get_text().strip()
            if len(text) > 5 and "è«‹æŒ‰æ­¤" not in text and "åŸæ–‡ç¶²å€" not in text:
                full_text_list.append(text)
        
        content_text = "\n\n".join(full_text_list)
        if len(content_text) < 20: content_text = "ï¼ˆç„¡æ³•è‡ªå‹•æå–è©³ç´°å…§æ–‡ï¼Œè«‹é»æ“Šé€£çµæŸ¥çœ‹ï¼‰"
        
        return content_text, real_url # å›å‚³å…§æ–‡å’ŒçœŸå¯¦ç¶²å€

    except Exception as e:
        return f"æŠ“å–å¤±æ•— ({str(e)})", real_url

def is_new_news(published_time_str):
    try:
        pub_time = datetime.datetime.strptime(published_time_str, '%Y-%m-%d %H:%M')
        pub_time = HK_TZ.localize(pub_time)
        now = datetime.datetime.now(HK_TZ)
        diff = (now - pub_time).total_seconds() / 60
        return 0 <= diff <= 30
    except:
        return False

@st.cache_data(ttl=60)
def fetch_news_data(func_name, *args):
    if func_name == "fetch_hk01":
        return fetch_hk01()
    elif func_name == "fetch_google_rss":
        return fetch_google_rss(*args)
    elif func_name == "fetch_direct_rss":
        return fetch_direct_rss(*args)
    return []

def fetch_google_rss(site_domain, site_name, color):
    """
    æ³¨æ„ï¼šGoogle RSS è¿”å›çš„æ˜¯è·³è½‰é€£çµã€‚
    ç‚ºäº†é é¢åŠ è¼‰é€Ÿåº¦ï¼Œæˆ‘å€‘åœ¨åˆ—è¡¨é æš«æ™‚é¡¯ç¤º Google é€£çµï¼Œ
    ä½†åœ¨ç”Ÿæˆ TXT æ™‚æœƒé€²è¡Œé‚„åŸã€‚
    """
    query = urllib.parse.quote(f"site:{site_domain}")
    rss_url = f"https://news.google.com/rss/search?q={query}+when:1d&hl=zh-HK&gl=HK&ceid=HK:zh-Hant"
    feed = feedparser.parse(rss_url)
    news = []
    for entry in feed.entries[:8]:
        title = entry.title.rsplit(" - ", 1)[0]
        dt_str = ""
        if hasattr(entry, 'published_parsed'):
            dt_obj = datetime.datetime.fromtimestamp(time.mktime(entry.published_parsed), UTC_TZ).astimezone(HK_TZ)
            dt_str = dt_obj.strftime('%Y-%m-%d %H:%M')
        news.append({'source': site_name, 'title': title, 'link': entry.link, 'time': dt_str, 'color': color})
    return news

def fetch_direct_rss(url, name, color):
    """ 
    ç›´æ¥æŠ“å–åª’é«”çš„ RSSï¼Œé€™æ¨£å¯ä»¥å¾—åˆ°çœŸå¯¦ç¶²å€ã€‚
    """
    try:
        r = requests.get(url, headers=HEADERS, timeout=8)
        feed = feedparser.parse(r.content)
        news = []
        for entry in feed.entries[:8]:
            dt_str = ""
            if hasattr(entry, 'published_parsed'):
                dt_obj = datetime.datetime.fromtimestamp(time.mktime(entry.published_parsed), UTC_TZ).astimezone(HK_TZ)
                dt_str = dt_obj.strftime('%Y-%m-%d %H:%M')
            elif hasattr(entry, 'updated_parsed'): # éƒ¨åˆ† RSS ä½¿ç”¨ updated
                 dt_obj = datetime.datetime.fromtimestamp(time.mktime(entry.updated_parsed), UTC_TZ).astimezone(HK_TZ)
                 dt_str = dt_obj.strftime('%Y-%m-%d %H:%M')
            
            link = entry.link
            # Now æ–°èç‰¹æ®Šè™•ç†ï¼šå¦‚æœé€£çµæ˜¯ç›¸å°è·¯å¾‘æˆ–ä¸å®Œæ•´
            if 'news.now.com' in url and 'http' not in link:
                 # Now RSS æœ‰æ™‚æœƒçµ¦å‡º weird çš„ linkï¼Œå˜—è©¦ä¿®å¾©
                 pass 
            
            news.append({'source': name, 'title': entry.title, 'link': link, 'time': dt_str, 'color': color})
        return news
    except Exception as e:
        return []

def fetch_hk01():
    try:
        r = requests.get("https://web-data.api.hk01.com/v2/feed/category/0", headers=HEADERS, timeout=5)
        items = r.json().get('items', [])[:8]
        news = []
        for item in items:
            raw = item.get('data', {})
            dt_str = datetime.datetime.fromtimestamp(raw.get('publishTime'), HK_TZ).strftime('%Y-%m-%d %H:%M')
            news.append({'source': "HK01", 'title': raw.get('title'), 'link': raw.get('publishUrl'), 'time': dt_str, 'color': "#184587"})
        return news
    except: return []

# --- 3. åˆå§‹åŒ–ç‹€æ…‹ ---

if 'selected_links' not in st.session_state:
    st.session_state.selected_links = set()
if 'generated_text' not in st.session_state:
    st.session_state.generated_text = ""
if 'all_current_news' not in st.session_state:
    st.session_state.all_current_news = []

# --- 4. å´é‚Šæ¬„æ§åˆ¶èˆ‡é¡¯ç¤º ---

with st.sidebar:
    st.title("âš™ï¸ æ§åˆ¶å°")
    
    col_btn1, col_btn2 = st.columns(2)
    with col_btn1:
        if st.button("ğŸ”„ åˆ·æ–°æ–°è"):
            st.cache_data.clear()
            st.session_state.all_current_news = []
            st.rerun()
    with col_btn2:
        if st.button("ğŸ—‘ï¸ æ¸…ç©ºé¸æ“‡"):
            st.session_state.selected_links = set()
            st.session_state.generated_text = ""
            st.rerun()

    st.divider()
    st.write(f"å·²é¸æ–°è: **{len(st.session_state.selected_links)}** ç¯‡")

    if st.button("ğŸ“„ ç”Ÿæˆ TXT å…§å®¹", type="primary", use_container_width=True):
        if not st.session_state.selected_links:
            st.warning("è«‹å…ˆåœ¨å³å´å‹¾é¸æ–°èï¼")
        else:
            with st.spinner("æ­£åœ¨æŠ“å–å…§æ–‡ä¸¦è§£æçœŸå¯¦ç¶²å€..."):
                final_txt = ""
                selected_items = [
                    item for item in st.session_state.all_current_news 
                    if item['link'] in st.session_state.selected_links
                ]
                
                count = 1
                total = len(selected_items)
                progress_bar = st.progress(0)

                for idx, item in enumerate(selected_items):
                    # åœ¨é€™è£¡åŒæ™‚ç²å– å…§æ–‡ å’Œ çœŸå¯¦ç¶²å€
                    content, real_url = fetch_full_article(item['link'])
                    
                    final_txt += f"ã€æ–°è {idx+1}ã€‘{item['source']}ï¼š{item['title']}\n"
                    final_txt += f"ç™¼å¸ƒæ™‚é–“ï¼š{item['time']}\n"
                    final_txt += "-" * 20 + "\n"
                    final_txt += f"{content}\n\n"
                    # ä½¿ç”¨é‚„åŸå¾Œçš„ real_url
                    final_txt += f"é€£çµï¼š{real_url}\n"
                    final_txt += "Ends\n\n" + "="*30 + "\n\n"
                    progress_bar.progress((idx + 1) / total)

                st.session_state.generated_text = final_txt
                progress_bar.empty()

    if st.session_state.generated_text:
        st.markdown("---")
        st.success("âœ… ç”Ÿæˆå®Œæˆï¼é€£çµå·²é‚„åŸç‚ºåŸå§‹ç¶²å€ã€‚")
        st.text_area("TXT å…§å®¹é è¦½", value=st.session_state.generated_text, height=600)

# --- 5. ä¸»ä»‹é¢ï¼šæ–°èé¡¯ç¤ºå€ ---

st.title("Tommy Sir å¾Œæ´æœƒä¹‹æ–°èç›£å¯Ÿç³»çµ±")
st.caption(f"æœ€å¾ŒåŒæ­¥æ™‚é–“: {datetime.datetime.now(HK_TZ).strftime('%Y-%m-%d %H:%M:%S')}")

# é‡é»ä¿®æ”¹ï¼šNow æ–°èæ”¹ç”¨ fetch_direct_rss
sources_config = [
    # ç¬¬ä¸€æ¬„
    [
        ("HK01", "fetch_hk01", []),
        ("é¦™æ¸¯é›»å°", "fetch_direct_rss", ["https://rthk.hk/rthk/news/rss/c_expressnews_clocal.xml", "é¦™æ¸¯é›»å°", "#FF9800"]),
    ],
    # ç¬¬äºŒæ¬„
    [
        # TVB æ²’æœ‰å®˜æ–¹ RSSï¼Œåªèƒ½ç”¨ Googleï¼Œä½†åœ¨ç”Ÿæˆ TXT æ™‚æˆ‘å€‘æœƒé‚„åŸç¶²å€
        ("ç„¡ç·šæ–°è (TVB)", "fetch_google_rss", ["news.tvb.com/tc/local", "ç„¡ç·šæ–°è", "#27ae60"]),
        ("æœ‰ç·šæ–°è", "fetch_direct_rss", ["https://www.i-cable.com/feed/", "æœ‰ç·šæ–°è", "#c0392b"]),
    ],
    # ç¬¬ä¸‰æ¬„
    [
        # Now æ–°èæ”¹ç”¨å®˜æ–¹ RSSï¼Œé€™æ¨£ç¶²å€å°±æ˜¯ news.now.com äº†
        ("Now æ–°è", "fetch_direct_rss", ["https://news.now.com/home/local/rss.xml", "Now æ–°è", "#E65100"]),
        # å•†å°æ²’æœ‰å®˜æ–¹ RSSï¼Œåªèƒ½ç”¨ Google
        ("å•†æ¥­é›»å°", "fetch_google_rss", ["881903.com", "å•†å°", "#F1C40F"]),
    ]
]

cols = st.columns(3)
temp_all_news = []

for col_idx, column_sources in enumerate(sources_config):
    with cols[col_idx]:
        for name, func_name, args in column_sources:
            st.markdown(f"<div class='news-source-header'>{name}</div>", unsafe_allow_html=True)
            news_items = fetch_news_data(func_name, *args)
            temp_all_news.extend(news_items)
            
            if not news_items:
                st.info("æš«ç„¡æ›´æ–°")
            else:
                for item in news_items:
                    link = item['link']
                    is_new = is_new_news(item['time'])
                    is_selected = link in st.session_state.selected_links
                    
                    c1, c2 = st.columns([0.1, 0.9])
                    with c1:
                        if st.checkbox("", key=f"chk_{link}", value=is_selected):
                            st.session_state.selected_links.add(link)
                        else:
                            st.session_state.selected_links.discard(link)
                    
                    with c2:
                        new_tag = '<span class="new-badge">NEW!</span>' if is_new else ''
                        text_style = 'class="read-text"' if is_selected else ""
                        
                        # é€™è£¡ä½¿ç”¨ HTML æ¸²æŸ“ï¼Œç¢ºä¿å¼•è™Ÿå’Œçµæ§‹æ­£ç¢º
                        # å³ä½¿åˆ—è¡¨é  TVB é¡¯ç¤ºçš„æ˜¯ google ç¶²å€ï¼Œç”Ÿæˆ TXT æ™‚æœƒè®ŠæˆçœŸå¯¦ç¶²å€
                        st.markdown(f"""
                            <div style="line-height:1.4; margin-bottom:10px;">
                                {new_tag}
                                <a href="{link}" target="_blank" {text_style}>
                                    {item['title']}
                                </a>
                                <br>
                                <span style="font-size:0.8em; color:#888;">{item['time']}</span>
                            </div>
                        """, unsafe_allow_html=True)

st.session_state.all_current_news = temp_all_news

