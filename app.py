# -*- coding: utf-8 -*-
import datetime
import re
import sys
from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple
from urllib.parse import quote_plus
from xml.etree import ElementTree as ET

import feedparser
import pytz
import requests
import streamlit as st
from bs4 import BeautifulSoup
from streamlit_autorefresh import st_autorefresh

# -----------------------
# Runtime / Encoding
# -----------------------
try:
    sys.stdout.reconfigure(encoding="utf-8")
except Exception:
    pass

HK_TZ = pytz.timezone("Asia/Hong_Kong")


def now_hk() -> datetime.datetime:
    return datetime.datetime.now(HK_TZ)


def today_hk_date() -> datetime.date:
    return now_hk().date()


def is_today_hk(dt_obj: datetime.datetime) -> bool:
    return dt_obj.astimezone(HK_TZ).date() == today_hk_date()


# -----------------------
# Streamlit Page Config
# -----------------------
st.set_page_config(page_title="Tommy Sirå¾Œæ´æœƒä¹‹æ–°èä¸­å¿ƒ", layout="wide", page_icon="ğŸ“°")

# -----------------------
# CSS (fixed-height panels to align horizontally)
# -----------------------
PANEL_HEIGHT_PX = 760

st.markdown(
    f"""
<style>
body {{ font-family: "Microsoft JhengHei", "PingFang TC", sans-serif; }}

.section-wrap {{ padding: 16px; border-radius: 12px; margin-bottom: 12px; }}
.section-gov {{ background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%); }}
.section-core {{ background: #f8f9fa; }}

.source-header {{
  font-size: 1.02em; font-weight: 800;
  margin: 0 0 10px 0; padding: 8px 12px;
  background: linear-gradient(135deg, #4f46e5 0%, #7c3aed 100%);
  color: white; border-radius: 10px; display: inline-block;
}}

.panel {{
  height: {PANEL_HEIGHT_PX}px;
  overflow-y: auto;
  padding-right: 6px;
}}
.panel::-webkit-scrollbar {{ width: 10px; }}
.panel::-webkit-scrollbar-thumb {{ background: #cbd5e1; border-radius: 999px; }}
.panel::-webkit-scrollbar-track {{ background: transparent; }}

.news-item {{
  padding: 10px 12px; margin: 6px 0;
  background: white; border-left: 5px solid #3498db;
  border-radius: 10px; transition: all 0.18s ease;
  box-shadow: 0 1px 3px rgba(0,0,0,0.06);
}}
.news-item:hover {{
  transform: translateX(3px);
  box-shadow: 0 3px 10px rgba(0,0,0,0.10);
  border-left-color: #ef4444;
}}
.news-title {{
  font-size: 0.97rem; font-weight: 650;
  color: #111827; text-decoration: none;
  line-height: 1.45; display: block; margin-bottom: 4px;
}}
.news-title:hover {{ color: #ef4444; }}
.news-meta {{
  font-size: 0.83rem; color: #6b7280;
  font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
}}

.badge-new {{
  display:inline-block; margin-left:8px; padding:2px 8px;
  border-radius:999px; background:#ef4444; color:white;
  font-size:0.75rem; font-weight:800;
}}
.badge-warn {{
  display:inline-block; margin-left:8px; padding:2px 8px;
  border-radius:999px; background:#b45309; color:white;
  font-size:0.75rem; font-weight:800;
}}

.small-note {{ color:#92400e; font-size:0.88rem; margin:-4px 0 10px 0; }}
hr {{ border: none; border-top: 1px solid #e5e7eb; margin: 12px 0; }}
</style>
""",
    unsafe_allow_html=True,
)

# -----------------------
# Data model
# -----------------------
@dataclass
class Article:
    source: str
    title: str
    link: str
    timestamp: datetime.datetime
    time_str: str
    color: str
    is_new: bool = False


# -----------------------
# Helpers
# -----------------------
def clean_text(raw: str) -> str:
    if not raw:
        return ""
    soup = BeautifulSoup(raw, "html.parser")
    text = soup.get_text(" ", strip=True)
    text = re.sub(r"<.*?>", "", text)
    return " ".join(text.split())


def safe_get(url: str, timeout: int = 14) -> requests.Response:
    headers = {
        # mimic mobile-ish UA to encourage mobile list markup
        "User-Agent": (
            "Mozilla/5.0 (iPhone; CPU iPhone OS 17_0 like Mac OS X) "
            "AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Mobile/15E148 Safari/604.1"
        ),
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "zh-HK,zh;q=0.9,en;q=0.7",
    }
    return requests.get(url, headers=headers, timeout=timeout, allow_redirects=True)


def parse_entry_time_from_feed(entry) -> Tuple[datetime.datetime, str]:
    struct_time = None
    if hasattr(entry, "published_parsed") and entry.published_parsed:
        struct_time = entry.published_parsed
    elif hasattr(entry, "updated_parsed") and entry.updated_parsed:
        struct_time = entry.updated_parsed

    if struct_time:
        dt_utc = datetime.datetime(*struct_time[:6], tzinfo=pytz.utc)
        dt_hk = dt_utc.astimezone(HK_TZ)
        return dt_hk, dt_hk.strftime("%H:%M")

    dt = now_hk()
    return dt, "--:--"


def extract_meta_time(html: str) -> Optional[datetime.datetime]:
    soup = BeautifulSoup(html, "html.parser")
    candidates: List[str] = []

    for prop in ["article:published_time", "og:updated_time", "article:modified_time"]:
        tag = soup.find("meta", attrs={"property": prop})
        if tag and tag.get("content"):
            candidates.append(tag["content"])

    for name in ["pubdate", "publishdate", "date", "parsely-pub-date"]:
        tag = soup.find("meta", attrs={"name": name})
        if tag and tag.get("content"):
            candidates.append(tag["content"])

    t = soup.find("time")
    if t and t.get("datetime"):
        candidates.append(t["datetime"])

    for s in candidates:
        s2 = s.strip().replace("Z", "+00:00")
        try:
            dt = datetime.datetime.fromisoformat(s2)
            if dt.tzinfo is None:
                dt = HK_TZ.localize(dt)
            return dt.astimezone(HK_TZ)
        except Exception:
            continue
    return None


def extract_title(html: str) -> str:
    soup = BeautifulSoup(html, "html.parser")
    ogt = soup.find("meta", attrs={"property": "og:title"})
    if ogt and ogt.get("content"):
        return clean_text(ogt["content"])
    if soup.title and soup.title.string:
        return clean_text(soup.title.string)
    h1 = soup.find("h1")
    if h1:
        return clean_text(h1.get_text(" ", strip=True))
    return ""


def parse_relative_zh_time(text: str) -> Optional[datetime.datetime]:
    """
    Parse strings like:
      - "17åˆ†é˜å‰" / "2å°æ™‚å‰"
      - "ä»Šå¤© 08:15" / "ä»Šæ—¥ 08:15"
      - "2025-12-29 08:15" / "2025/12/29 08:15"
    Return HK datetime.
    """
    if not text:
        return None
    s = clean_text(text)

    # minutes ago
    m = re.search(r"(\d+)\s*åˆ†é˜å‰", s)
    if m:
        mins = int(m.group(1))
        return now_hk() - datetime.timedelta(minutes=mins)

    # hours ago
    h = re.search(r"(\d+)\s*å°æ™‚å‰", s)
    if h:
        hrs = int(h.group(1))
        return now_hk() - datetime.timedelta(hours=hrs)

    # today HH:MM
    t = re.search(r"(ä»Šæ—¥|ä»Šå¤©)\s*(\d{{1,2}}):(\d{{2}})", s)
    if t:
        hh = int(t.group(2))
        mm = int(t.group(3))
        dt = datetime.datetime.combine(today_hk_date(), datetime.time(hh, mm))
        return HK_TZ.localize(dt)

    # YYYY-MM-DD HH:MM or YYYY/MM/DD HH:MM
    d = re.search(r"(\d{4})[-/](\d{1,2})[-/](\d{1,2})\s+(\d{1,2}):(\d{2})", s)
    if d:
        yy, mo, da, hh, mm = map(int, d.groups())
        dt = datetime.datetime(yy, mo, da, hh, mm)
        return HK_TZ.localize(dt)

    return None


def mark_new_and_remember(source_key: str, items: List[Article]) -> List[Article]:
    if "seen_links" not in st.session_state:
        st.session_state["seen_links"] = {}  # type: ignore
    seen: Dict[str, set] = st.session_state["seen_links"]  # type: ignore
    if source_key not in seen:
        seen[source_key] = set()

    for it in items:
        it.is_new = it.link not in seen[source_key]
    for it in items:
        seen[source_key].add(it.link)
    return items


def render_articles(articles: List[Article], warn_non_official: bool = False) -> str:
    if not articles:
        return "<p style='color:#9ca3af; padding:14px; text-align:center;'>ä»Šæ—¥æš«ç„¡æ–°è</p>"

    html = "<div class='panel'>"
    for a in articles:
        new_badge = ' <span class="badge-new">NEW</span>' if a.is_new else ""
        warn_badge = ' <span class="badge-warn">éå®˜æ–¹èšåˆ</span>' if warn_non_official else ""
        html += f"""
        <div class="news-item" style="border-left-color:{a.color};">
            <a class="news-title" href="{a.link}" target="_blank" rel="noopener noreferrer">{a.title}</a>
            <div class="news-meta">ğŸ• {a.time_str} Â· {a.source}{new_badge}{warn_badge}</div>
        </div>
        """
    html += "</div>"
    return html


# -----------------------
# Fetchers (today only, limit N)
# -----------------------
def fetch_rss_today(source_key: str, source_name: str, url: str, color: str, limit: int = 10) -> List[Article]:
    out: List[Article] = []
    try:
        feed = feedparser.parse(url)
        entries = getattr(feed, "entries", None) or []
        for entry in entries:
            title = clean_text(getattr(entry, "title", ""))
            link = getattr(entry, "link", "")
            if not title or not link:
                continue

            dt_obj, time_str = parse_entry_time_from_feed(entry)
            if not is_today_hk(dt_obj):
                continue

            out.append(Article(source=source_name, title=title, link=link, timestamp=dt_obj, time_str=time_str, color=color))
            if len(out) >= limit:
                break
    except Exception as e:
        st.warning(f"[RSS] {source_name} è®€å–å¤±æ•—ï¼š{e}")

    out.sort(key=lambda x: x.timestamp, reverse=True)
    return mark_new_and_remember(source_key, out[:limit])


def fetch_google_news_today(source_key: str, source_name: str, query: str, color: str, limit: int = 10) -> List[Article]:
    url = (
        "https://news.google.com/rss/search?q="
        + quote_plus(query)
        + "&hl=zh-HK&gl=HK&ceid=HK:zh-Hant"
    )
    return fetch_rss_today(source_key, source_name, url, color, limit=limit)


def fetch_tvb_news_sitemap_today(source_key: str, source_name: str, sitemap_url: str, color: str, limit: int = 10) -> List[Article]:
    """
    TVB /instant ç‚º JS renderï¼Œç›´æ¥æŠ“ sitemap.xmlï¼ˆNews sitemapï¼‰æœ€ç©©ã€‚
    """
    out: List[Article] = []
    try:
        resp = safe_get(sitemap_url, timeout=16)
        resp.raise_for_status()
        root = ET.fromstring(resp.text)

        ns = {
            "sm": "http://www.sitemaps.org/schemas/sitemap/0.9",
            "news": "http://www.google.com/schemas/sitemap-news/0.9",
        }

        for url_node in root.findall("sm:url", ns)[:400]:
            loc = url_node.findtext("sm:loc", default="", namespaces=ns).strip()
            title = url_node.findtext("news:news/news:title", default="", namespaces=ns).strip()
            pub = url_node.findtext("news:news/news:publication_date", default="", namespaces=ns).strip()

            if not loc or not title or not pub:
                continue

            try:
                dt0 = datetime.datetime.fromisoformat(pub.replace("Z", "+00:00"))
                if dt0.tzinfo is None:
                    dt0 = HK_TZ.localize(dt0)
                dt = dt0.astimezone(HK_TZ)
            except Exception:
                continue

            if not is_today_hk(dt):
                continue

            out.append(Article(source=source_name, title=clean_text(title), link=loc, timestamp=dt, time_str=dt.strftime("%H:%M"), color=color))
            if len(out) >= limit:
                break

    except Exception as e:
        st.warning(f"[TVB sitemap] è®€å–å¤±æ•—ï¼š{e}")

    out.sort(key=lambda x: x.timestamp, reverse=True)
    return mark_new_and_remember(source_key, out[:limit])


def fetch_list_page_links(base_url: str, html: str, link_pattern: re.Pattern) -> List[str]:
    soup = BeautifulSoup(html, "html.parser")
    links: List[str] = []
    for a in soup.select("a[href]"):
        href = (a.get("href") or "").strip()
        if not href:
            continue
        if href.startswith("/"):
            href = base_url.rstrip("/") + href
        if link_pattern.search(href):
            links.append(href)

    # dedup keep order
    seen = set()
    dedup: List[str] = []
    for x in links:
        if x not in seen:
            seen.add(x)
            dedup.append(x)
    return dedup


def fetch_hk01_latest_today_html(source_key: str, source_name: str, list_url: str, color: str, limit: int = 10) -> List[Article]:
    """
    HK01ï¼šä¸ç”¨ JSONï¼Œç›´æ¥æŠ“ /latest åˆ—è¡¨ï¼Œå†é€æ¢å…¥æ–‡ç« å– meta time + titleã€‚
    """
    out: List[Article] = []
    try:
        resp = safe_get(list_url, timeout=16)
        resp.raise_for_status()

        # HK01 article urls often like https://www.hk01.com/article/123456 æˆ– /æ¸¯è/123456/xxx
        link_pattern = re.compile(r"hk01\.com/(article/\d+|[^/]+/\d+)", re.IGNORECASE)
        candidates = fetch_list_page_links("https://www.hk01.com", resp.text, link_pattern)[:160]

        for link in candidates:
            try:
                page = safe_get(link, timeout=12)
                if page.status_code != 200:
                    continue
                dt = extract_meta_time(page.text)
                if not dt or not is_today_hk(dt):
                    continue
                title = extract_title(page.text)
                if not title:
                    continue

                out.append(Article(source=source_name, title=title, link=link, timestamp=dt, time_str=dt.strftime("%H:%M"), color=color))
                if len(out) >= limit:
                    break
            except Exception:
                continue

    except Exception as e:
        st.warning(f"[HK01 HTML] è®€å–å¤±æ•—ï¼š{e}")

    out.sort(key=lambda x: x.timestamp, reverse=True)
    return mark_new_and_remember(source_key, out[:limit])


def fetch_topick_news_today_html(source_key: str, source_name: str, list_url: str, color: str, limit: int = 10) -> List[Article]:
    """
    TOPickï¼šæŠ“ã€Œæ–°èã€åˆ—è¡¨é ï¼Œå†é€æ¢å…¥æ–‡å– meta time + titleã€‚
    """
    out: List[Article] = []
    try:
        resp = safe_get(list_url, timeout=16)
        resp.raise_for_status()

        # hket / topick articles often have /article/ or /srat...; accept a broad pattern
        link_pattern = re.compile(r"topick\.hket\.com/(article/\d+|.+/\d+)", re.IGNORECASE)
        candidates = fetch_list_page_links("https://topick.hket.com", resp.text, link_pattern)[:180]

        for link in candidates:
            try:
                page = safe_get(link, timeout=12)
                if page.status_code != 200:
                    continue
                dt = extract_meta_time(page.text)
                if not dt:
                    # fallback: try parse relative text from page
                    dt = parse_relative_zh_time(page.text)
                if not dt or not is_today_hk(dt):
                    continue
                title = extract_title(page.text)
                if not title:
                    continue

                out.append(Article(source=source_name, title=title, link=link, timestamp=dt, time_str=dt.strftime("%H:%M"), color=color))
                if len(out) >= limit:
                    break
            except Exception:
                continue

    except Exception as e:
        st.warning(f"[TOPick HTML] è®€å–å¤±æ•—ï¼š{e}")

    out.sort(key=lambda x: x.timestamp, reverse=True)
    return mark_new_and_remember(source_key, out[:limit])


def fetch_dotdotnews_immed_today_html(source_key: str, source_name: str, list_url: str, color: str, limit: int = 10) -> List[Article]:
    """
    é»æ–°èï¼šæŠ“ /immed åˆ—è¡¨ï¼Œç›´æ¥å¾åˆ—è¡¨æŠ½ (title, link, time)ï¼›å¦‚ time æ˜¯ã€Œxxåˆ†é˜å‰ã€å°±æ›ç®—ã€‚
    """
    out: List[Article] = []
    try:
        resp = safe_get(list_url, timeout=16)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, "html.parser")

        # broad: all internal article links
        items = []
        for a in soup.select("a[href]"):
            href = (a.get("href") or "").strip()
            if not href:
                continue
            if href.startswith("/"):
                href = "https://www.dotdotnews.com" + href
            if "dotdotnews.com" not in href:
                continue
            # avoid navigation
            if any(x in href for x in ["/immed", "/category", "/tag", "/search", "/video"]):
                continue

            title = clean_text(a.get_text(" ", strip=True))
            if not title or len(title) < 6:
                continue

            # try get nearby time text (same card)
            time_text = ""
            parent = a.parent
            if parent:
                time_text = clean_text(parent.get_text(" ", strip=True))

            dt = parse_relative_zh_time(time_text) or now_hk()
            if not is_today_hk(dt):
                continue

            items.append((title, href, dt))

        # dedup by link, keep newest dt
        best: Dict[str, Tuple[str, datetime.datetime]] = {}
        for title, link, dt in items:
            if link not in best or dt > best[link][1]:
                best[link] = (title, dt)

        articles = [
            Article(source=source_name, title=t, link=l, timestamp=dt, time_str=dt.strftime("%H:%M") if dt else "--:--", color=color)
            for l, (t, dt) in best.items()
        ]
        articles.sort(key=lambda x: x.timestamp, reverse=True)
        out = articles[:limit]

    except Exception as e:
        st.warning(f"[é»æ–°è HTML] è®€å–å¤±æ•—ï¼š{e}")

    return mark_new_and_remember(source_key, out[:limit])


def fetch_tkww_top_news_today_html(source_key: str, source_name: str, list_url: str, color: str, limit: int = 10) -> List[Article]:
    """
    å¤§å…¬æ–‡åŒ¯ï¼ˆtkwwï¼‰ï¼š/top_news åˆ—è¡¨å…§é€šå¸¸æœ‰ã€Œxxåˆ†é˜å‰ã€ç­‰ç›¸å°æ™‚é–“ã€‚
    """
    out: List[Article] = []
    try:
        resp = safe_get(list_url, timeout=16)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, "html.parser")

        candidates: List[Article] = []
        for a in soup.select("a[href]"):
            href = (a.get("href") or "").strip()
            if not href:
                continue
            if href.startswith("/"):
                href = "https://www.tkww.hk" + href
            if "tkww.hk" not in href:
                continue
            if any(x in href for x in ["/top_news", "/topic", "/search", "/video"]):
                continue

            title = clean_text(a.get_text(" ", strip=True))
            if not title or len(title) < 6:
                continue

            # try parse relative time from surrounding block
            time_text = ""
            block = a.parent
            if block:
                time_text = clean_text(block.get_text(" ", strip=True))

            dt = parse_relative_zh_time(time_text)
            if not dt:
                # fallback: open article page meta time (slower; do for limited items)
                try:
                    page = safe_get(href, timeout=12)
                    if page.status_code == 200:
                        dt = extract_meta_time(page.text)
                except Exception:
                    dt = None

            if not dt or not is_today_hk(dt):
                continue

            candidates.append(Article(source=source_name, title=title, link=href, timestamp=dt, time_str=dt.strftime("%H:%M"), color=color))

        # dedup by link
        dedup = {}
        for a in candidates:
            if a.link not in dedup or a.timestamp > dedup[a.link].timestamp:
                dedup[a.link] = a

        out = list(dedup.values())
        out.sort(key=lambda x: x.timestamp, reverse=True)
        out = out[:limit]

    except Exception as e:
        st.warning(f"[tkww HTML] è®€å–å¤±æ•—ï¼š{e}")

    return mark_new_and_remember(source_key, out[:limit])


def fetch_telegram_channel_today(source_key: str, source_name: str, channel_public_url: str, color: str, limit: int = 10) -> List[Article]:
    """
    å•†æ¥­é›»å° Telegram é »é“ï¼ˆå…¬é–‹é ï¼‰ï¼šhttps://t.me/s/<channel>
    å–æ¯å‰‡ message çš„é€£çµï¼ˆè‹¥æœ‰ï¼‰ï¼Œä¸¦ç”¨ã€Œä»Šæ—¥ã€åˆ¤æ–·ï¼ˆTelegram é é¢æ™‚é–“æ ¼å¼å¯èƒ½è®Šå‹•ï¼Œæ¡ä¿å®ˆç­–ç•¥ï¼‰
    """
    out: List[Article] = []
    try:
        resp = safe_get(channel_public_url, timeout=16)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, "html.parser")

        # Telegram web public messages
        # message blocks: div.tgme_widget_message_wrap or div.tgme_widget_message
        blocks = soup.select("div.tgme_widget_message_wrap, div.tgme_widget_message")
        for b in blocks:
            # link to the post
            post_link = ""
            a = b.select_one("a.tgme_widget_message_date")
            if a and a.get("href"):
                post_link = a["href"]

            # message text
            msg = b.select_one("div.tgme_widget_message_text")
            text = clean_text(msg.decode_contents() if msg else "")
            if not text:
                continue

            # Telegram shows time in title attribute sometimes
            dt = None
            if a and a.get("title"):
                # title example might be "2025-12-29 10:33:00"
                dt = parse_relative_zh_time(a.get("title", ""))

            # fallback: treat as today if the page is current and we cannot parse (conservative)
            if dt is None:
                dt = now_hk()

            if not is_today_hk(dt):
                continue

            # choose first URL in message as news link if present
            urls = re.findall(r"https?://[^\s)]+", text)
            link = urls[0] if urls else (post_link or channel_public_url)

            # title: first line trimmed
            title = text.split("\n")[0].strip()
            title = re.sub(r"\s+", " ", title)
            if len(title) > 70:
                title = title[:70] + "â€¦"

            out.append(Article(source=source_name, title=title, link=link, timestamp=dt, time_str=dt.strftime("%H:%M"), color=color))
            if len(out) >= limit:
                break

    except Exception as e:
        st.warning(f"[Telegram] {source_name} è®€å–å¤±æ•—ï¼š{e}")

    out.sort(key=lambda x: x.timestamp, reverse=True)
    return mark_new_and_remember(source_key, out[:limit])


# -----------------------
# Cache wrapper (60s)
# -----------------------
@st.cache_data(ttl=60, show_spinner=False)
def cached(kind: str, args: Tuple):
    if kind == "rss_today":
        return fetch_rss_today(*args)
    if kind == "google_today":
        return fetch_google_news_today(*args)
    if kind == "tvb_sitemap":
        return fetch_tvb_news_sitemap_today(*args)
    if kind == "hk01_html":
        return fetch_hk01_latest_today_html(*args)
    if kind == "topick_html":
        return fetch_topick_news_today_html(*args)
    if kind == "dotdot_html":
        return fetch_dotdotnews_immed_today_html(*args)
    if kind == "tkww_html":
        return fetch_tkww_top_news_today_html(*args)
    if kind == "telegram_today":
        return fetch_telegram_channel_today(*args)
    return []


# -----------------------
# UI Header + Auto refresh
# -----------------------
st.title("ğŸ—ï¸ Tommy Sirå¾Œæ´æœƒä¹‹æ–°èä¸­å¿ƒ")
st.caption(f"åªé¡¯ç¤ºä»Šæ—¥æ–°èï¼ˆé¦™æ¸¯æ™‚é–“ï¼‰ï½œæœ€å¾Œæ›´æ–°ï¼š{now_hk().strftime('%Y-%m-%d %H:%M:%S')}")

top_a, top_b, top_c = st.columns([1, 1, 2])
with top_a:
    limit_each = st.selectbox("æ¯å€‹åª’é«”é¡¯ç¤º", [10], index=0)
with top_b:
    auto_on = st.toggle("â±ï¸ æ¯åˆ†é˜è‡ªå‹•æ›´æ–°", value=True)
with top_c:
    st.markdown(
        "<div class='small-note'>NEWï¼šä»£è¡¨æœ¬æ¬¡é‹è¡Œé¦–æ¬¡è¦‹åˆ°çš„é€£çµï¼ˆåŒä¸€å€‹ session å…§æœƒè¨˜ä½å·²å‡ºç¾éçš„é€£çµï¼‰ã€‚</div>",
        unsafe_allow_html=True,
    )

if auto_on:
    st_autorefresh(interval=60 * 1000, key="auto_refresh_60s")

if st.button("ğŸ”„ ç«‹å³åˆ·æ–°", type="primary"):
    st.cache_data.clear()
    st.rerun()

st.markdown("<hr/>", unsafe_allow_html=True)

# -----------------------
# Government (ZH/EN separate)
# -----------------------
st.markdown('<div class="section-wrap section-gov">', unsafe_allow_html=True)
st.markdown("### ğŸ›ï¸ æ”¿åºœæ–°èèˆ‡å…¬å‘Šï¼ˆä¸­ / è‹±åˆ†é–‹ï½œå„ 10 æ¢ï½œåªé¡¯ç¤ºä»Šæ—¥ï¼‰")

gov_zh_col, gov_en_col = st.columns(2)
with gov_zh_col:
    st.markdown('<div class="source-header">ğŸ›ï¸ æ”¿åºœæ–°èï¼ˆä¸­æ–‡ï¼‰</div>', unsafe_allow_html=True)
    gov_zh = cached("rss_today", ("gov_zh", "æ”¿åºœæ–°èï¼ˆä¸­æ–‡ï¼‰", "https://www.info.gov.hk/gia/rss/general_zh.xml", "#E74C3C", limit_each))
    st.markdown(render_articles(gov_zh), unsafe_allow_html=True)

with gov_en_col:
    st.markdown('<div class="source-header">ğŸ›ï¸ Gov News (English)</div>', unsafe_allow_html=True)
    gov_en = cached("rss_today", ("gov_en", "Gov News (English)", "https://www.info.gov.hk/gia/rss/general_en.xml", "#C0392B", limit_each))
    st.markdown(render_articles(gov_en), unsafe_allow_html=True)

st.markdown("</div>", unsafe_allow_html=True)
st.markdown("<hr/>", unsafe_allow_html=True)

# -----------------------
# Media sources (5 columns aligned)
# -----------------------
NEG_ENT = "-å¨›æ¨‚ -æ¼”å”±æœƒ -éŸ³æ¨‚ -æ­Œæ‰‹ -é›»å½± -æ˜æ˜Ÿ -ç¶œè— -åŠ‡é›† -é ’çç¦® -èŠ±é‚Š -å…«å¦ -KOL -æ—…éŠ -ç¾é£Ÿ"
BASE_NEWS_HINT = "(æ–°è OR æ¸¯è OR æœ¬åœ° OR æ™‚äº‹ OR æ”¿åºœ OR ç«‹æ³•æœƒ OR è­¦æ–¹ OR æ³•åº­ OR äº¤é€š OR å¤©æ°£ OR ç¶“æ¿Ÿ OR è²¡ç¶“)"

MEDIA_SOURCES = [
    # key, display_name, kind, payload, warn_non_official
    ("rthk", "RTHKï¼ˆæœ¬åœ° RSSï¼‰", "rss_today",
     ("rthk", "RTHKï¼ˆæœ¬åœ°ï¼‰", "https://rthk.hk/rthk/news/rss/c_expressnews_clocal.xml", "#FF9800", limit_each),
     False),

    # å•†æ¥­é›»å°ï¼šå„ªå…ˆ Telegramï¼ˆä½ æä¾›çš„é »é“ï¼‰ï¼Œfallback Google Newsï¼ˆä»æ’é™¤å¨›æ¨‚ï¼‰
    ("cr_tg", "å•†æ¥­é›»å°ï¼ˆTelegramï¼‰", "telegram_today",
     ("cr_tg", "å•†æ¥­é›»å°ï¼ˆTelegramï¼‰", "https://t.me/s/cr881903", "#F59E0B", limit_each),
     False),
    ("cr_gn", "å•†æ¥­é›»å°ï¼ˆæ–°èéæ¿¾ãƒ»å‚™æ´ï¼‰", "google_today",
     ("cr_gn", "å•†æ¥­é›»å°ï¼ˆå‚™æ´ï¼‰", '(881903 OR "å•†æ¥­é›»å°" OR "å±å’¤903") ' + BASE_NEWS_HINT + " " + NEG_ENT, "#B45309", limit_each),
     True),

    # HK01ï¼šæ”¹ HTML /latest çœŸçˆ¬èŸ²
    ("hk01", "HK01ï¼ˆHTML å³æ™‚ï¼‰", "hk01_html",
     ("hk01", "HK01ï¼ˆå³æ™‚ï¼‰", "https://www.hk01.com/latest", "#1F4E79", limit_each),
     False),

    # Nowï¼šä½ è‹¥ä»è¦ Nowï¼Œæˆ‘ä¿ç•™ Google Newsï¼ˆNow ç«™æ”¹ç‰ˆæœƒæ¯”è¼ƒå¸¸åçˆ¬ï¼›ä½ ä¹‹å‰ Now HTML äº¦å¯èƒ½å‡ºç¾ 403ï¼‰
    ("now", "Nowï¼ˆèšåˆï¼‰", "google_today",
     ("now", "Nowï¼ˆèšåˆï¼‰", 'site:news.now.com ' + BASE_NEWS_HINT + " " + NEG_ENT, "#3B82F6", limit_each),
     True),

    # TVBï¼šæ”¹æŠ“ sitemapï¼ˆNews sitemapï¼‰
    ("tvb", "TVBï¼ˆsitemap å³æ™‚ï¼‰", "tvb_sitemap",
     ("tvb", "TVBï¼ˆå³æ™‚ï¼‰", "https://news.tvb.com/sitemap.xml", "#10B981", limit_each),
     False),

    # æ˜å ±ï¼šå®˜æ–¹ RSSï¼ˆå³æ™‚æ–°èï¼‰
    ("mingpao", "æ˜å ±ï¼ˆå®˜æ–¹å³æ™‚ RSSï¼‰", "rss_today",
     ("mingpao", "æ˜å ±ï¼ˆå³æ™‚ï¼‰", "https://news.mingpao.com/rss/ins/all.xml", "#6B7280", limit_each),
     False),

    # TOPickï¼šæ”¹ HTML çœŸçˆ¬èŸ²ï¼ˆæ–°èé ï¼‰
    ("topick", "TOPickï¼ˆHTML æ–°èï¼‰", "topick_html",
     ("topick", "TOPickï¼ˆæ–°èï¼‰", "https://topick.hket.com/srat006/%E6%96%B0%E8%81%9E", "#6B7280", limit_each),
     False),

    # é»æ–°èï¼šæ”¹ HTML çœŸçˆ¬èŸ²ï¼ˆ/immedï¼‰
    ("dotdot", "é»æ–°èï¼ˆHTML å³æ™‚ï¼‰", "dotdot_html",
     ("dotdot", "é»æ–°èï¼ˆå³æ™‚ï¼‰", "https://www.dotdotnews.com/immed", "#6B7280", limit_each),
     False),

    # å¤§å…¬æ–‡åŒ¯ï¼ˆtkwwï¼‰ï¼šæ”¹ HTML çœŸçˆ¬èŸ²ï¼ˆ/top_newsï¼‰
    ("tkww", "å¤§å…¬æ–‡åŒ¯ï¼ˆHTML å³æ™‚ï¼‰", "tkww_html",
     ("tkww", "å¤§å…¬æ–‡åŒ¯ï¼ˆå³æ™‚ï¼‰", "https://www.tkww.hk/top_news", "#6B7280", limit_each),
     False),

    # å…¶ä»–ä½ åå–®å…§ï¼šå…ˆç”¨èšåˆï¼ˆä½ ä¹‹å¾Œè¦é€å€‹å†å‡ç´šï¼‰
    ("onc", "on.ccï¼ˆèšåˆï¼‰", "google_today",
     ("onc", "on.ccï¼ˆèšåˆï¼‰", 'site:on.cc ' + BASE_NEWS_HINT + " " + NEG_ENT, "#6B7280", limit_each),
     True),
    ("singtao", "æ˜Ÿå³¶ï¼ˆèšåˆï¼‰", "google_today",
     ("singtao", "æ˜Ÿå³¶ï¼ˆèšåˆï¼‰", 'site:stheadline.com ' + BASE_NEWS_HINT + " " + NEG_ENT, "#6B7280", limit_each),
     True),
    ("hkej", "ä¿¡å ±å³æ™‚ï¼ˆèšåˆï¼‰", "google_today",
     ("hkej", "ä¿¡å ±å³æ™‚ï¼ˆèšåˆï¼‰", 'site:hkej.com ' + BASE_NEWS_HINT + " " + NEG_ENT, "#6B7280", limit_each),
     True),
    ("cable", "Cable å³æ™‚ï¼ˆèšåˆï¼‰", "google_today",
     ("cable", "Cableï¼ˆèšåˆï¼‰", 'site:i-cable.com ' + BASE_NEWS_HINT + " " + NEG_ENT, "#6B7280", limit_each),
     True),
    ("hkcd", "é¦™æ¸¯å•†å ±ï¼ˆèšåˆï¼‰", "google_today",
     ("hkcd", "é¦™æ¸¯å•†å ±ï¼ˆèšåˆï¼‰", 'site:hkcd.com ' + BASE_NEWS_HINT + " " + NEG_ENT, "#6B7280", limit_each),
     True),
    ("wenweipo", "æ–‡åŒ¯å ±ï¼ˆèšåˆï¼‰", "google_today",
     ("wenweipo", "æ–‡åŒ¯å ±ï¼ˆèšåˆï¼‰", 'site:wenweipo.com ' + BASE_NEWS_HINT + " " + NEG_ENT, "#6B7280", limit_each),
     True),
]

st.markdown('<div class="section-wrap section-core">', unsafe_allow_html=True)
st.markdown("### ğŸ“° ä»Šæ—¥æ–°èï¼ˆæ¯å€‹å¹³å° 10 æ¢ï½œ5 æ¬„ä¸¦æ’å°é½Šï½œåªé¡¯ç¤ºä»Šæ—¥ï¼‰")
st.markdown(
    "<div class='small-note'>å·²å‡ç´šï¼šHK01 / TVB / TOPick / é»æ–°è / å¤§å…¬æ–‡åŒ¯ï¼ˆæ”¹ç”¨åˆ—è¡¨é çœŸçˆ¬èŸ²æˆ– sitemapï¼‰ã€‚å•†æ¥­é›»å°å„ªå…ˆ Telegramã€‚å…¶é¤˜ä»æ¨™ç¤ºã€Œéå®˜æ–¹èšåˆã€è€…ï¼Œä¹‹å¾Œå¯æŒ‰ç«™é€å€‹å†å‡ç´šã€‚</div>",
    unsafe_allow_html=True,
)

cols = st.columns(5)
for idx, (key, name, kind, payload, warn_non_official) in enumerate(MEDIA_SOURCES):
    with cols[idx % 5]:
        st.markdown(f'<div class="source-header">ğŸ“° {name}</div>', unsafe_allow_html=True)
        with st.spinner("è®€å–ä¸­..."):
            items = cached(kind, payload)
            st.markdown(render_articles(items, warn_non_official=warn_non_official), unsafe_allow_html=True)

st.markdown("</div>", unsafe_allow_html=True)

st.caption(
    "æç¤ºï¼šè‹¥æŸç«™å‡ºç¾ 403/åçˆ¬ï¼Œé€šå¸¸éœ€è¦é™ä½æŠ“å–é »ç‡ã€èª¿æ•´ headersã€æˆ–æ”¹ç”¨ Playwrightã€‚"
)
