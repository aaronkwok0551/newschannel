# app.py
# -*- coding: utf-8 -*-

import datetime
import html
import sys
from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple

import feedparser
import pytz
import requests
import streamlit as st
from bs4 import BeautifulSoup
from dateutil import parser as dtparser
from streamlit_autorefresh import st_autorefresh
from textwrap import dedent

# =====================
# åŸºæœ¬è¨­å®š
# =====================
try:
    sys.stdout.reconfigure(encoding="utf-8")
except Exception:
    pass

HK_TZ = pytz.timezone("Asia/Hong_Kong")

st.set_page_config(page_title="Tommy Sirå¾Œæ´æœƒä¹‹æ–°èä¸­å¿ƒ", layout="wide", page_icon="ğŸ—ï¸")

# =====================
# CSSï¼ˆåŒ…å«ï¼šæ–°æ–°è 20 åˆ†é˜ç´…è‰²ï¼‰
# =====================
st.markdown(
    dedent(
        """
        <style>
        body { font-family: "Microsoft JhengHei","PingFang TC",sans-serif; }

        .section-title{
          font-size:1.05rem;font-weight:800;margin:2px 0 8px 0;
        }

        .card{
          background:#fff;border:1px solid #e5e7eb;border-radius:12px;
          padding:12px;height:520px;display:flex;flex-direction:column;
        }

        .items{ overflow-y:auto; padding-right:6px; flex:1; }

        .item{
          background:#fff;border-left:4px solid #3b82f6;border-radius:8px;
          padding:8px 10px;margin:8px 0;
        }

        /* æ–°æ–°èï¼ˆ20 åˆ†é˜å…§ï¼‰ */
        .item.new{
          border-left-color:#ef4444 !important;
          box-shadow: 0 0 0 1px rgba(239,68,68,0.25);
        }
        .item.new a{ color:#b91c1c !important; }

        .item a{
          text-decoration:none;color:#111827;font-weight:600;line-height:1.35;
        }
        .item a:hover{ color:#ef4444; }

        .item-meta{
          font-size:0.78rem;color:#6b7280;font-family:monospace;margin-top:2px;
        }

        .empty{ color:#9ca3af;text-align:center;margin-top:20px; }
        .warn{ color:#b45309;font-size:0.85rem;margin:6px 0 0 0; }
        </style>
        """
    ),
    unsafe_allow_html=True,
)

# =====================
# Model
# =====================
@dataclass
class Article:
    title: str
    link: str
    dt: Optional[datetime.datetime]  # HK time
    time_str: str
    color: str
    is_new: bool = False

# =====================
# Helpers
# =====================
def now_hk() -> datetime.datetime:
    return datetime.datetime.now(HK_TZ)

def clean_text(raw: str) -> str:
    """æŠŠ RSS/JSON è£¡çš„ HTML è½‰ç´”æ–‡å­—ï¼›div class æœ¬èº«å””æœƒå†å‡ºç¾ã€‚"""
    raw = html.unescape(raw or "")
    soup = BeautifulSoup(raw, "html.parser")
    text = soup.get_text(" ", strip=True)
    # å†ä¿å®ˆæ¸…ç†ä¸€ä¸‹
    return " ".join(text.split())

def parse_time_from_entry(entry) -> Optional[datetime.datetime]:
    """feedparser æ™‚é–“è§£æï¼ˆæœ‰å°±è½‰ HK timeï¼Œå†‡å°± Noneï¼‰"""
    if getattr(entry, "published_parsed", None):
        return datetime.datetime(*entry.published_parsed[:6], tzinfo=pytz.utc).astimezone(HK_TZ)
    if getattr(entry, "updated_parsed", None):
        return datetime.datetime(*entry.updated_parsed[:6], tzinfo=pytz.utc).astimezone(HK_TZ)

    for key in ("published", "updated", "pubDate", "date"):
        val = getattr(entry, key, None)
        if val:
            try:
                dt = dtparser.parse(str(val))
                if dt.tzinfo is None:
                    dt = HK_TZ.localize(dt)
                return dt.astimezone(HK_TZ)
            except Exception:
                pass
    return None

def chunked(lst: List, n: int) -> List[List]:
    return [lst[i:i+n] for i in range(0, len(lst), n)]

def mark_new_by_first_seen(articles: List[Article], window_minutes: int = 20) -> None:
    """
    ç”¨ã€Œç¬¬ä¸€æ¬¡è¦‹åˆ°è©² link çš„æ™‚é–“ã€åˆ¤æ–·æ–°æ–°èï¼Œ
    ä¸ä¾è³´ feed çš„ publish timeï¼ˆå› ç‚ºå¥½å¤š RSS ä¿‚å†‡/å””æº–ï¼‰ã€‚
    """
    if "seen_links" not in st.session_state:
        st.session_state["seen_links"] = {}  # link -> first_seen_iso

    seen: Dict[str, str] = st.session_state["seen_links"]
    now = now_hk()
    window = datetime.timedelta(minutes=window_minutes)

    for a in articles:
        if not a.link:
            continue
        if a.link not in seen:
            seen[a.link] = now.isoformat()

        try:
            first_seen = dtparser.parse(seen[a.link])
            if first_seen.tzinfo is None:
                first_seen = HK_TZ.localize(first_seen)
            first_seen = first_seen.astimezone(HK_TZ)
            a.is_new = (now - first_seen) <= window
        except Exception:
            a.is_new = False

    st.session_state["seen_links"] = seen

# =====================
# Fetchers
# =====================
DEFAULT_HEADERS = {
    "User-Agent": "Mozilla/5.0 (Streamlit; HK News Aggregator)",
    "Accept": "*/*",
}

@st.cache_data(ttl=60)
def fetch_rss(url: str, color: str, limit: int = 12) -> Tuple[List[Article], Optional[str]]:
    """
    é€šç”¨ RSS / RSSHubï¼š
    - å–æœ€æ–° limit
    - ç›¡é‡è§£ææ™‚é–“ï¼ˆèƒ½æ’å°±æ’ï¼‰
    - title/summary å…¨éƒ¨è½‰ç´”æ–‡å­—ï¼ˆå””æœƒå†å‡ºç¾ div classï¼‰
    """
    try:
        r = requests.get(url, timeout=15, headers=DEFAULT_HEADERS)
        r.raise_for_status()

        feed = feedparser.parse(r.content)
        if not feed.entries:
            return [], "æœªæœ‰ entriesï¼ˆå¯èƒ½ä¾†æºæš«æ™‚ç„¡æ›´æ–°ï¼æˆ– RSSHub è·¯ç”±è®Šæ›´ï¼‰"

        out: List[Article] = []
        for e in feed.entries[: (limit * 3)]:  # å¤šæŠ“å°‘å°‘å†ç¯©
            title = clean_text(getattr(e, "title", "") or "")
            link = getattr(e, "link", "") or ""
            if not title or not link:
                continue

            dt = parse_time_from_entry(e)
            time_str = dt.strftime("%H:%M") if dt else "â€”"

            out.append(Article(title=title, link=link, dt=dt, time_str=time_str, color=color))
            if len(out) >= limit:
                break

        if not out:
            return [], "æœ‰ entries ä½†æœªèƒ½æŠ½å–åˆ°æœ‰æ•ˆ title/link"
        return out, None

    except requests.HTTPError as e:
        return [], f"HTTPError: {e}"
    except Exception as e:
        return [], f"{type(e).__name__}: {e}"

@st.cache_data(ttl=60)
def fetch_now_api(color: str, limit: int = 12) -> Tuple[List[Article], Optional[str]]:
    """
    Nowï¼ˆæœ¬åœ°ï¼‰ç”¨ APIï¼š
    https://newsapi1.now.com/pccw-news-api/api/getNewsListv2?category=119&pageNo=1

    æ³¨æ„ï¼šNow å›å‚³ JSON å…§æŸäº›æ¬„ä½å« HTML ä¿‚æ­£å¸¸ï¼›æˆ‘å“‹åªæŠ½ title/link/timeã€‚
    """
    NOW_URL = "https://newsapi1.now.com/pccw-news-api/api/getNewsListv2"
    params = {"category": 119, "pageNo": 1}

    try:
        r = requests.get(NOW_URL, params=params, timeout=15, headers=DEFAULT_HEADERS)
        r.raise_for_status()

        data = r.json()
        # Now å¯èƒ½ä¿‚ list æˆ– dictï¼›ä½ è²¼éä¿‚ list[dict]
        candidates = None
        if isinstance(data, list):
            candidates = data
        elif isinstance(data, dict):
            # ä¿å®ˆæƒ key
            for k in ("data", "list", "news", "items", "result"):
                v = data.get(k)
                if isinstance(v, list):
                    candidates = v
                    break
            if candidates is None:
                # å†æƒä¸€å±¤
                for v in data.values():
                    if isinstance(v, list):
                        candidates = v
                        break
                    if isinstance(v, dict):
                        for kk in ("data", "list", "news", "items", "result"):
                            vv = v.get(kk)
                            if isinstance(vv, list):
                                candidates = vv
                                break
                    if candidates is not None:
                        break

        if not candidates:
            return [], "Now API å›å‚³çµæ§‹å·²è®Šï¼ˆæ‰¾ä¸åˆ°æ–°èåˆ—è¡¨ï¼‰"

        out: List[Article] = []
        for it in candidates:
            if not isinstance(it, dict):
                continue

            title = clean_text(str(it.get("title") or it.get("newsTitle") or it.get("headline") or ""))
            news_id = str(it.get("newsId") or "").strip()

            # linkï¼šç”¨ now ç¶²ç«™ playerï¼ˆæœ€ç©©å®šï¼‰
            link = ""
            if news_id:
                link = f"https://news.now.com/home/local/player?newsId={news_id}"
            else:
                raw = str(it.get("shareUrl") or it.get("url") or it.get("link") or "").strip()
                if raw.startswith("/"):
                    raw = "https://news.now.com" + raw
                link = raw

            # publishDate å¤šæ•¸ä¿‚ epoch ms
            dt = None
            time_str = "â€”"
            raw_time = it.get("publishDate") or it.get("publishTime") or it.get("publishedAt") or it.get("date")
            if raw_time is not None:
                try:
                    if isinstance(raw_time, (int, float)) or str(raw_time).isdigit():
                        ts = int(raw_time)
                        # å¦‚æœä¿‚æ¯«ç§’
                        if ts > 10_000_000_000:
                            ts = ts // 1000
                        dt = datetime.datetime.fromtimestamp(ts, tz=HK_TZ)
                    else:
                        dt = dtparser.parse(str(raw_time))
                        if dt.tzinfo is None:
                            dt = HK_TZ.localize(dt)
                        dt = dt.astimezone(HK_TZ)
                    time_str = dt.strftime("%H:%M") if dt else "â€”"
                except Exception:
                    dt = None
                    time_str = "â€”"

            if title and link:
                out.append(Article(title=title, link=link, dt=dt, time_str=time_str, color=color))
            if len(out) >= limit:
                break

        if not out:
            return [], "Now API æœ‰å›å‚³ä½†æœªèƒ½æŠ½å–åˆ°æœ‰æ•ˆæ–°èé …ç›®"
        return out, None

    except requests.HTTPError as e:
        return [], f"HTTPError: {e}"
    except Exception as e:
        return [], f"{type(e).__name__}: {e}"

# =====================
# Renderï¼ˆé—œéµï¼šé¿å…ç¸®æ’è®Šæˆé»‘è‰² code blockï¼‰
# =====================
def build_card_html(title: str, articles: List[Article], warn: Optional[str] = None) -> str:
    if not articles:
        items_html = "<div class='empty'>æš«ç„¡å…§å®¹</div>"
    else:
        parts = []
        for a in articles:
            new_class = " new" if a.is_new else ""
            parts.append(
                f"""<div class="item{new_class}" style="border-left-color:{a.color}">
<a href="{a.link}" target="_blank" rel="noopener noreferrer">{html.escape(a.title)}</a>
<div class="item-meta">ğŸ• {html.escape(a.time_str)}</div>
</div>"""
            )
        items_html = "".join(parts)

    warn_html = f"<div class='warn'>âš ï¸ {html.escape(warn)}</div>" if warn else ""

    # é‡è¦ï¼šdedent + stripï¼Œé¿å… Markdown èª¤åˆ¤ç‚º code block
    return dedent(
        f"""
        <div class="section-title">{html.escape(title)}</div>
        <div class="card">
          <div class="items">
            {items_html}
          </div>
          {warn_html}
        </div>
        """
    ).strip()

def sort_articles_desc(articles: List[Article]) -> List[Article]:
    # æœ‰ dt çš„æ’å‰é¢ï¼›å†‡ dt çš„ä¿æŒç›¸å°é †åºï¼ˆé åŸ feed æœ€æ–°ï¼‰
    with_dt = [a for a in articles if a.dt is not None]
    without_dt = [a for a in articles if a.dt is None]
    with_dt.sort(key=lambda x: x.dt, reverse=True)
    return with_dt + without_dt

# =====================
# URLsï¼ˆä½ å¯è‡ªç”±åŠ æ¸›ï¼›Now ç”¨ API ç‰¹åˆ¥è™•ç†ï¼‰
# =====================
GOV_ZH = "https://www.info.gov.hk/gia/rss/general_zh.xml"
GOV_EN = "https://www.info.gov.hk/gia/rss/general_en.xml"
RTHK = "https://rthk.hk/rthk/news/rss/c_expressnews_clocal.xml"

# =====================
# UI
# =====================
st.title("ğŸ—ï¸ Tommy Sirå¾Œæ´æœƒä¹‹æ–°èä¸­å¿ƒ")
st.caption(f"æœ€å¾Œæ›´æ–°ï¼ˆé¦™æ¸¯æ™‚é–“ï¼‰ï¼š{now_hk().strftime('%Y-%m-%d %H:%M:%S')}")

# RSSHub baseï¼ˆä½ è©±å””çŸ¥ template nameï¼›å‘¢åº¦ç›´æ¥ç”¨ URLï¼‰
rsshub_base = st.sidebar.text_input(
    "RSSHub Base URLï¼ˆä¾‹å¦‚ https://rsshub-production-xxxx.up.railway.appï¼‰",
    value="https://rsshub-production-9dfc.up.railway.app",
).rstrip("/")

auto = st.toggle("æ¯åˆ†é˜è‡ªå‹•æ›´æ–°", value=True)
if auto:
    st_autorefresh(interval=60_000, key="auto")

limit = st.sidebar.slider("æ¯å€‹ä¾†æºé¡¯ç¤ºå¹¾å¤šæ¢", 5, 30, 12, 1)

# ä½ æŒ‡å®šçš„åª’é«”ï¼ˆå¯å†åŠ ï¼‰
sources = [
    {"name": "æ”¿åºœæ–°èï¼ˆä¸­æ–‡ï¼‰", "type": "rss", "url": GOV_ZH, "color": "#E74C3C"},
    {"name": "æ”¿åºœæ–°èï¼ˆè‹±æ–‡ï¼‰", "type": "rss", "url": GOV_EN, "color": "#C0392B"},
    {"name": "RTHK", "type": "rss", "url": RTHK, "color": "#FF9800"},

    # Nowï¼šç‰¹åˆ¥è™•ç†ï¼ˆå””ç”¨ RSSHubï¼Œé¿å…å£è·¯ç”±ï¼‰
    {"name": "Now æ–°èï¼ˆæœ¬åœ°ï¼‰", "type": "now_api", "url": "", "color": "#16A34A"},

    # ä½ æä¾›çš„ RSSHub routesï¼ˆæ³¨æ„ï¼šNow RSSHub ä½ è©±å£å’—ï¼Œæ‰€ä»¥å””ç”¨ï¼‰
    {"name": "HK01", "type": "rss", "url": f"{rsshub_base}/hk01/latest", "color": "#2563EB"},
    {"name": "on.cc æ±ç¶²", "type": "rss", "url": f"{rsshub_base}/oncc/zh-hant/news", "color": "#7C3AED"},
    {"name": "æ˜Ÿå³¶å³æ™‚", "type": "rss", "url": f"https://www.stheadline.com/rss", "color": "#F97316"},
    {"name": "æ˜å ±å³æ™‚", "type": "rss", "url": f"https://news.mingpao.com/rss/ins/all.xml", "color": "#7C3AED"},
    {"name": "i-CABLE æœ‰ç·š", "type": "rss", "url": f"{https://www.i-cable.com/feed", "color": "#A855F7"},
    {"name": "ç¶“æ¿Ÿæ—¥å ±", "type": "rss", "url": f"https://www.hket.com/rss/hongkong", "color": "#7C3AED"},
    {"name": "ä¿¡å ±å³æ™‚", "type": "rss", "url": f"{rsshub_base}/hkej/index", "color": "#64748B"},
    {"name": "å·´å£«çš„å ±", "type": "rss", "url": f"https://www.bastillepost.com/hongkong/feed", "color": "#7C3AED"},
    {"name": "TVB æ–°è", "type": "rss", "url": f"{rsshub_base}/tvb/news/tc", "color": "#0EA5E9"},



]

# ä½ å¯ä»¥æ—¥å¾Œå†åŠ ï¼ˆæ˜å ±å®˜æ–¹ RSS ä½ è©±ã€Œå®˜æ–¹ RSSã€ï¼Œä½ æœªæä¾› URLï¼Œä¹‹å¾Œè£œä¸Šå³å¯ï¼‰

# Renderï¼šæ¯è¡Œ 4 æ¬„ï¼ˆä¿æŒæ©«å‘ä¸¦åˆ—ï¼‰
cols_per_row = 4
rows = chunked(sources, cols_per_row)

for row in rows:
    cols = st.columns(len(row))
    for col, src in zip(cols, row):
        with col:
            if src["type"] == "now_api":
                arts, warn = fetch_now_api(src["color"], limit=limit)
            else:
                arts, warn = fetch_rss(src["url"], src["color"], limit=limit)

            # æ–°èæ¨™ç´…ï¼ˆ20 åˆ†é˜ï¼‰
            mark_new_by_first_seen(arts, window_minutes=20)

            # æ¯å€‹å¹³å°å…§éƒ¨æŒ‰æ™‚é–“æ–°åˆ°èˆŠ
            arts = sort_articles_desc(arts)

            st.markdown(build_card_html(src["name"], arts, warn=warn), unsafe_allow_html=True)



