# -*- coding: utf-8 -*-
import streamlit as st
import requests
import feedparser
import datetime
import pytz
import urllib.parse
import time
from bs4 import BeautifulSoup
import sys
from streamlit_autorefresh import st_autorefresh

# è¨­å®šé è¨­ç·¨ç¢¼ä»¥é˜²æ­¢ä¸­æ–‡äº‚ç¢¼
try:
    sys.stdout.reconfigure(encoding='utf-8')
except:
    pass

# --- 1. é é¢è¨­å®š ---
st.set_page_config(
    page_title="HK News Hub",
    page_icon="ğŸ“°",
    layout="wide"
)

# --- è¨­å®šè‡ªå‹•åˆ·æ–° (æ¯ 60 ç§’ = 60000 æ¯«ç§’) ---
count = st_autorefresh(interval=60 * 1000, limit=None, key="news_autoupdate")

# --- è‡ªè¨‚ CSS (å„ªåŒ–ä¸¦æ’é¡¯ç¤ºèˆ‡ Checkbox) ---
st.markdown("""
<style>
    .stCheckbox { margin-bottom: 0px; }
    /* ä¾†æºæ¨™é¡Œæ¨£å¼ */
    .news-source-header { 
        font-size: 1.1em; 
        font-weight: bold; 
        color: #2c3e50; 
        margin-top: 15px; 
        margin-bottom: 10px;
        padding-bottom: 5px;
        border-bottom: 2px solid #eee;
    }
    /* é€£çµæ¨£å¼ */
    a { text-decoration: none; color: #2980b9; }
    a:hover { text-decoration: underline; color: #e74c3c; }
    
    /* èª¿æ•´ Checkbox èˆ‡æ–‡å­—å°é½Š */
    div[data-testid="column"] { display: flex; align-items: start; }
    
    /* å¾®èª¿å®¹å™¨æ¨£å¼ */
    div.block-container { padding-top: 2rem; }
</style>
""", unsafe_allow_html=True)

# è¨­å®šæ™‚å€
HK_TZ = pytz.timezone('Asia/Hong_Kong')
UTC_TZ = pytz.timezone('UTC')

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
}

# --- 2. æ ¸å¿ƒåŠŸèƒ½å‡½å¼ ---

def resolve_google_url(url):
    """é‚„åŸ Google News çœŸå¯¦é€£çµ"""
    if "news.google.com" not in url:
        return url
    try:
        # ä½¿ç”¨ HEAD è«‹æ±‚å¿«é€Ÿç²å–çœŸå¯¦ç¶²å€
        r = requests.head(url, headers=HEADERS, allow_redirects=True, timeout=5)
        return r.url
    except:
        return url

@st.cache_data(ttl=60)
def fetch_via_google_news(site_domain, site_name, color, query_suffix=""):
    """Google News ä»£ç†æŠ“å–"""
    query = f"site:{site_domain} {query_suffix}".strip()
    encoded_query = urllib.parse.quote(query)
    # when:1d é™åˆ¶ç‚ºéå» 24 å°æ™‚
    rss_url = f"https://news.google.com/rss/search?q={encoded_query}+when:1d&hl=zh-HK&gl=HK&ceid=HK:zh-Hant"
    
    try:
        feed = feedparser.parse(rss_url)
        news_list = []
        for entry in feed.entries[:10]: # å–å‰ 10 æ¢
            # ç§»é™¤ Google News æ¨™é¡Œå¾Œé¢çš„ä¾†æºå¾Œç¶´ (ä¾‹å¦‚ " - Source Name")
            title = entry.title.rsplit(" - ", 1)[0] if " - " in entry.title else entry.title
            
            dt_str = ""
            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                try:
                    dt_obj = datetime.datetime.fromtimestamp(time.mktime(entry.published_parsed), UTC_TZ).astimezone(HK_TZ)
                    dt_str = dt_obj.strftime('%Y-%m-%d %H:%M')
                except:
                    dt_str = "æœ€æ–°"

            summary = entry.get('summary', '') or entry.get('description', 'ç„¡æ‘˜è¦å…§å®¹')
            soup = BeautifulSoup(summary, "html.parser")
            clean_summary = soup.get_text().strip()

            news_list.append({
                'source': site_name,
                'title': title,
                'link': entry.link,
                'time': dt_str,
                'content': clean_summary,
                'color': color
            })
        return news_list
    except:
        return []

@st.cache_data(ttl=60)
def fetch_hk01_api():
    """HK01 API"""
    try:
        url = "https://web-data.api.hk01.com/v2/feed/category/0"
        r = requests.get(url, headers=HEADERS, timeout=5)
        items = r.json().get('items', [])[:10]
        news_list = []
        for item in items:
            raw = item.get('data', {})
            ts = raw.get('publishTime')
            dt_str = datetime.datetime.fromtimestamp(ts, HK_TZ).strftime('%Y-%m-%d %H:%M') if ts else ""
            
            news_list.append({
                'source': "HK01",
                'title': raw.get('title'),
                'link': raw.get('publishUrl'),
                'time': dt_str,
                'content': raw.get('description') or "ç„¡æ‘˜è¦å…§å®¹",
                'color': "#184587"
            })
        return news_list
    except:
        return fetch_via_google_news("hk01.com", "HK01", "#184587")

@st.cache_data(ttl=60)
def fetch_direct_rss(url, name, color):
    """ç›´æ¥ RSS æŠ“å–"""
    try:
        r = requests.get(url, headers=HEADERS, timeout=5)
        feed = feedparser.parse(r.content)
        news_list = []
        for entry in feed.entries[:10]:
            dt_str = ""
            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                try:
                    dt_obj = datetime.datetime.fromtimestamp(time.mktime(entry.published_parsed), UTC_TZ).astimezone(HK_TZ)
                    dt_str = dt_obj.strftime('%Y-%m-%d %H:%M')
                except:
                    dt_str = "æœ€æ–°"
            
            summary = entry.get('summary', '') or entry.get('description', 'ç„¡æ‘˜è¦å…§å®¹')
            soup = BeautifulSoup(summary, "html.parser")
            clean_summary = soup.get_text().strip()

            link = entry.link
            if 'news.now.com' in url and 'news.now.com' not in link:
                link = f"https://news.now.com{link}"

            news_list.append({
                'source': name,
                'title': entry.title,
                'link': link,
                'time': dt_str,
                'content': clean_summary,
                'color': color
            })
        return news_list
    except:
        return []

# --- 3. ä¸»ç¨‹å¼ä»‹é¢ ---

st.title("ğŸ‡­ğŸ‡° é¦™æ¸¯å³æ™‚æ–°èä¸­å¿ƒ")
current_time = datetime.datetime.now(HK_TZ).strftime('%H:%M:%S')
st.caption(f"è‡ªå‹•æ›´æ–°ä¸­ (æ¯ 60 ç§’) | æœ€å¾Œæ›´æ–°: {current_time}")

# åˆå§‹åŒ– Session State (ç”¨ä¾†è¨˜æ†¶å‹¾é¸ç‹€æ…‹)
if 'selected_links' not in st.session_state:
    st.session_state.selected_links = set()

# å®šç¾©æ–°èä¾†æº
sources = [
    {"func": fetch_hk01_api, "args": [], "name": "HK01", "color": "#184587"},
    {"func": fetch_via_google_news, "args": ["news.tvb.com/tc/local", "ç„¡ç·šæ–°è", "#27ae60"], "name": "ç„¡ç·šæ–°è", "color": "#27ae60"},
    {"func": fetch_via_google_news, "args": ["news.now.com/home/local", "Nowæ–°è", "#E65100"], "name": "Now æ–°è", "color": "#E65100"},
    {"func": fetch_direct_rss, "args": ["https://rthk.hk/rthk/news/rss/c_expressnews_clocal.xml", "é¦™æ¸¯é›»å°", "#FF9800"], "name": "é¦™æ¸¯é›»å°", "color": "#FF9800"},
    {"func": fetch_direct_rss, "args": ["https://www.i-cable.com/feed/", "æœ‰ç·šæ–°è", "#c0392b"], "name": "æœ‰ç·šæ–°è", "color": "#c0392b"},
    {"func": fetch_via_google_news, "args": ["881903.com", "å•†å°", "#F1C40F"], "name": "å•†å° 881903", "color": "#F1C40F"},
]

# æ‰‹å‹•åˆ·æ–°æŒ‰éˆ•
if st.button("ğŸ”„ ç«‹å³åˆ·æ–°"):
    st.cache_data.clear()
    st.rerun()

# --- 4. é¡¯ç¤ºæ–°èå…§å®¹ (ä¸¦æ’ç‰ˆé¢ Grid Layout) ---
all_news_items = [] # ç”¨ä¾†æ”¶é›†æ‰€æœ‰æ–°èè³‡æ–™ï¼Œçµ¦ä¸‹è¼‰åŠŸèƒ½ä½¿ç”¨

# å»ºç«‹ 3 å€‹ä¸¦æ’æ¬„ä½
cols = st.columns(3)

# éæ­·æ‰€æœ‰ä¾†æº
for i, source_conf in enumerate(sources):
    # æ±ºå®šé€™å€‹ä¾†æºè¦æ”¾åœ¨å“ªä¸€æ¬„ (0, 1, 2 å¾ªç’°)
    col = cols[i % 3]
    
    with col:
        # åŸ·è¡ŒæŠ“å–å‡½æ•¸
        if source_conf["func"] == fetch_hk01_api:
            items = source_conf["func"]()
        else:
            items = source_conf["func"](*source_conf["args"])
            
        # é¡¯ç¤ºæ¨™é¡Œ
        st.markdown(f"<div class='news-source-header' style='border-left: 5px solid {source_conf['color']}; padding-left: 10px;'>{source_conf['name']}</div>", unsafe_allow_html=True)
        
        if not items:
            st.info("æš«ç„¡è³‡æ–™")
        else:
            # é¡¯ç¤ºè©²ä¾†æºçš„æ‰€æœ‰æ–°è
            for item in items:
                all_news_items.append(item) # å­˜å…¥å¤§åˆ—è¡¨
                
                # ä½¿ç”¨æ–°èé€£çµä½œç‚ºå”¯ä¸€ ID (é¿å…é †åºè®Šå‹•å°è‡´å‹¾é¸éŒ¯èª¤)
                unique_key = item['link']
                
                # ä½¿ç”¨å…©æ¬„ä½ˆå±€ï¼šå·¦é‚Šæ˜¯ Checkboxï¼Œå³é‚Šæ˜¯æ–°èå…§å®¹
                sub_col1, sub_col2 = st.columns([0.1, 0.9])
                
                with sub_col1:
                    # æª¢æŸ¥æ˜¯å¦å·²è¢«å‹¾é¸
                    is_checked = unique_key in st.session_state.selected_links
                    
                    # å®šç¾© Callback å‡½æ•¸ä¾†æ›´æ–°ç‹€æ…‹
                    def update_selection(key=unique_key):
                        if key in st.session_state.selected_links:
                            st.session_state.selected_links.remove(key)
                        else:
                            st.session_state.selected_links.add(key)

                    # é¡¯ç¤º Checkbox (ç„¡æ¨™ç±¤)
                    st.checkbox("", key=f"chk_{unique_key}", value=is_checked, on_change=update_selection)
                
                with sub_col2:
                    # é¡¯ç¤ºæ–°èæ¨™é¡Œã€é€£çµèˆ‡æ™‚é–“
                    st.markdown(
                        f"**[{item['title']}]({item['link']})** <br><span style='font-size:0.8em; color:#888;'>{item['time']}</span>", 
                        unsafe_allow_html=True
                    )

# --- 5. å´é‚Šæ¬„ï¼šä¸‹è¼‰åŠŸèƒ½ ---
with st.sidebar:
    st.header("ğŸ—ƒï¸ æª”æ¡ˆç”Ÿæˆå€")
    count = len(st.session_state.selected_links)
    st.write(f"ç›®å‰å·²é¸æ“‡ï¼š **{count}** ç¯‡æ–°è")
    
    if count > 0:
        if st.button("ğŸ“„ ç”Ÿæˆ TXT æª”æ¡ˆ"):
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            output_text = ""
            processed_count = 0
            
            # å¾æ‰€æœ‰æ–°èä¸­æ‰¾å‡ºè¢«å‹¾é¸çš„é …ç›®
            selected_items_data = [item for item in all_news_items if item['link'] in st.session_state.selected_links]
            
            # é–‹å§‹ç”Ÿæˆå…§å®¹
            for item in selected_items_data:
                processed_count += 1
                status_text.text(f"è§£æé€£çµä¸­ ({processed_count}/{len(selected_items_data)}): {item['source']}...")
                progress_bar.progress(processed_count / len(selected_items_data))
                
                # å¦‚æœæ˜¯ Google News é€£çµï¼Œå˜—è©¦é‚„åŸçœŸå¯¦ç¶²å€
                real_link = item['link']
                if "news.google.com" in real_link:
                    real_link = resolve_google_url(real_link)
                
                # çµ„åˆ TXT æ ¼å¼ (ç¬¦åˆæ‚¨çš„è¦æ±‚)
                output_text += f"{item['source']}ï¼š{item['title']}\n"
                output_text += f"[{item['time']}]\n\n"
                output_text += f"{item['content']}\n\n"
                output_text += f"{real_link}\n\n"
                output_text += "Ends\n\n"
                
            status_text.success("âœ… ç”Ÿæˆå®Œæˆï¼")
            progress_bar.empty()
            
            # å»ºç«‹ä¸‹è¼‰æŒ‰éˆ•
            current_time_str = datetime.datetime.now(HK_TZ).strftime('%Y%m%d_%H%M')
            st.download_button(
                label="ğŸ“¥ ä¸‹è¼‰ .txt æª”æ¡ˆ",
                data=output_text,
                file_name=f"news_digest_{current_time_str}.txt",
                mime="text/plain"
            )
    else:
        st.info("è«‹åœ¨å³å´å‹¾é¸æ–°èä»¥ç”Ÿæˆæ‘˜è¦ã€‚")
        if st.button("æ¸…é™¤æ‰€æœ‰é¸æ“‡"):
            st.session_state.selected_links.clear()
            st.rerun()
