#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
HK News Monitor - Uses MiniMax AI to determine relevant news
Deduplication and daily filtering enabled
"""

import requests
import feedparser
import datetime
import pytz
import os
import json
import time

# Hong Kong Timezone
HK_TZ = pytz.timezone('Asia/Hong_Kong')

# File to track sent articles
SENT_ARTICLES_FILE = 'sent_articles.txt'

# RSS Sources to monitor
RSS_SOURCES = {
    # Government RSS
    'æ”¿åºœæ–°è': 'https://www.info.gov.hk/gia/rss/general_zh.xml',
    
    # News Sources with AI filtering
    'HK01': 'https://news.hk01.com/rss/focus/2135',
    'on.cc': 'https://news.on.cc/hk/import/rdf/news.rdf',
    'nowæ–°è': 'https://news.now.com/home/rss.xml',
    'RTHK': 'https://rthk.hk/rthk/news/rss/c_expressnews_clocal.xml',
    'æ˜Ÿå³¶': 'https://www.stheadline.com/rss',
    'æ˜å ±': 'https://news.mingpao.com/rss/ins/all.xml',
}

def load_sent_articles():
    """Load previously sent article URLs"""
    sent = set()
    try:
        if os.path.exists(SENT_ARTICLES_FILE):
            with open(SENT_ARTICLES_FILE, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line.startswith('http'):
                        sent.add(line)
    except Exception as e:
        print(f"   âš ï¸ Error loading sent articles: {e}")
    return sent

def save_sent_articles(sent_urls):
    """Save sent article URLs to file"""
    try:
        with open(SENT_ARTICLES_FILE, 'w', encoding='utf-8') as f:
            for url in sorted(sent_urls):
                f.write(f"{url}\n")
    except Exception as e:
        print(f"   âš ï¸ Error saving sent articles: {e}")

def is_today(dt_obj):
    """Check if datetime is today in HKT"""
    now = datetime.datetime.now(HK_TZ)
    return dt_obj.date() == now.date()

def is_recent(dt_obj, hours=24):
    """Check if datetime is within specified hours"""
    now = datetime.datetime.now(HK_TZ)
    return (now - dt_obj).total_seconds() < (hours * 3600)

def send_telegram(message):
    """Send message to Telegram"""
    bot_token = os.environ.get('TELEGRAM_BOT_TOKEN')
    chat_id = os.environ.get('TELEGRAM_CHAT_ID')
    
    if not bot_token or not chat_id:
        print("âš ï¸ Telegram credentials not set")
        return False
    
    url = f"https://api.telegram.org/bot{bot_token}/sendMessage"
    data = {
        'chat_id': chat_id,
        'text': message,
        'parse_mode': 'Markdown',
        'disable_web_page_preview': True
    }
    
    try:
        response = requests.post(url, json=data, timeout=10)
        if response.ok:
            print("âœ… Telegram notification sent")
            return True
        else:
            print(f"âŒ Telegram failed: {response.text}")
    except Exception as e:
        print(f"âŒ Telegram error: {e}")
    return False

def check_with_minimax(title, source):
    """Use MiniMax AI to check if news is relevant"""
    api_key = os.environ.get('MINIMAX_API_KEY', '')
    
    # Regions to EXCLUDE
    exclude_regions = ['æ—¥æœ¬', 'å°ç£', 'ç æµ·', 'æ¾³é–€', 'æ¾³æ´²', 'ä¸­åœ‹', 'å…§åœ°', 'å¤§é™¸', 'æ·±åœ³', 'å»£å·', 'åŒ—äº¬', 'ä¸Šæµ·', 'æ³°åœ‹', 'é¦¬ä¾†è¥¿äº', 'æ–°åŠ å¡', 'éŸ“åœ‹', 'è‹±åœ‹', 'ç¾åœ‹', 'åŠ æ‹¿å¤§']
    
    # Very strict fallback keywords (must be HK-related)
    keywords = ['æ¯’å“', 'æµ·é—œ', 'ä¿å®‰å±€', 'é„§ç‚³å¼·', 'ç·æ¯’', 'å¤ªç©ºæ²¹', 'ä¾è¨—å’ªé…¯', 'ç¦æ¯’', 'èµ°ç§', 'æª¢ç²', 'æˆªç²', 'é¦™æ¸¯', 'æ¸¯å³¶', 'ä¹é¾', 'æ–°ç•Œ']
    
    # First check: exclude non-HK regions
    for region in exclude_regions:
        if region in title:
            print(f"   ğŸš« Excluded (non-HK region: {region})")
            return False
    
    if not api_key:
        print(f"   âš ï¸ MINIMAX_API_KEY not set!")
        result = any(kw in title for kw in keywords) and 'é¦™æ¸¯' in title
        print(f"   ğŸ” Keyword check (no AI): {result}")
        return result
    
    try:
        url = "https://api.minimax.chat/v1/text/chatcompletion_v2"
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        data = {
            "model": "MiniMax-M2.1",
            "messages": [
                {
                    "role": "system",
                    "content": "ä½ ä¿‚ä¸€å€‹åš´æ ¼æ—¢é¦™æ¸¯æ–°èç·¨è¼¯ã€‚éæ¿¾æ¨™æº–ï¼š\n1. åªæ¥å—ã€Œé¦™æ¸¯ã€æœ¬åœ°æ—¢æ¯’å“ã€æµ·é—œã€ä¿å®‰å±€æ–°è\n2. ä¸€æ—¦æ¨™é¡Œå‡ºç¾ã€Œæ—¥æœ¬ã€å°ç£ã€ç æµ·ã€æ¾³é–€ã€æ¾³æ´²ã€ä¸­åœ‹ã€å…§åœ°ã€å¤§é™¸ã€æ·±åœ³ã€å»£å·ã€å‘¢å•²åœ°å€ï¼Œå…¨éƒ¨éƒ½ä¿‚NO\n3. é¦™æ¸¯åœ°ç”¢ã€å¨›æ¨‚ã€æ”¿æ²»å…¶ä»–åœ°æ–¹æ–°èéƒ½ä¿‚NO\n4. é¦™æ¸¯æµ·é—œ/è­¦å¯Ÿ/ç·æ¯’æ—¢æ–°èå…ˆYES"
                },
                {
                    "role": "user",
                    "content": f"""åš´æ ¼åˆ¤æ–·å‘¢æ¢æ¨™é¡Œä¿‚å’ªã€Œé¦™æ¸¯æœ¬åœ°æ—¢æ¯’å“/æµ·é—œ/ä¿å®‰å±€ã€æ–°èï¼š

æ¨™é¡Œ: {title}
ä¾†æº: {source}

âŒ å¦‚æœæ¨™é¡Œæœ‰ä»¥ä¸‹æƒ…æ³ï¼Œå¿…é ˆç­”NOï¼š
- æåˆ°æ—¥æœ¬ã€å°ç£ã€ç æµ·ã€æ¾³é–€ã€æ¾³æ´²ã€ä¸­åœ‹ã€å…§åœ°ã€å¤§é™¸ç­‰éé¦™æ¸¯åœ°å€
- ç´”ç²¹é¦™æ¸¯åœ°ç”¢/æ¨“ç›¤
- é¦™æ¸¯å¨›æ¨‚åœˆ/TVB
- ä¸€èˆ¬é¦™æ¸¯ç¤¾æœƒæ–°èï¼ˆå””é—œæ¯’å“/æµ·é—œ/ä¿å®‰å±€ï¼‰

âœ… åªæœ‰å‘¢å•²å…ˆYESï¼š
- é¦™æ¸¯æœ¬åœ°æ¯’å“ç›¸é—œæ–°è
- é¦™æ¸¯æµ·é—œç·æ¯’/èµ°ç§æ–°è
- é¦™æ¸¯ä¿å®‰å±€/ç¦æ¯’è™•/è­¦å¯Ÿç·æ¯’æ–°è

è«‹åªå›ç­”ã€ŒYESã€æˆ–ã€ŒNOã€"""
                }
            ],
            "max_tokens": 10,
            "temperature": 0.1
        }
        
        print(f"   ğŸ”„ Calling MiniMax API...")
        response = requests.post(url, headers=headers, json=data, timeout=30)
        
        print(f"   ğŸ“¡ API response status: {response.status_code}")
        print(f"   ğŸ“ Raw response (first 300 chars): {response.text[:300]}")
        
        if response.status_code == 200:
            result = response.json()
            
            # Try different response formats
            answer = None
            
            # Format 1: OpenAI-style (choices)
            if 'choices' in result and len(result['choices']) > 0:
                answer = result['choices'][0]['message']['content'].strip().upper()
            
            # Format 2: MiniMax direct
            elif 'text' in result:
                answer = result['text'].strip().upper()
            
            # Format 3: Look for answer in base_resp
            elif 'base_resp' in result:
                text_content = result.get('base_resp', {}).get('text_content', '')
                if text_content:
                    answer = text_content.strip().upper()
            
            if answer:
                print(f"   ğŸ“ AI answer: {answer}")
                return answer == 'YES'
            else:
                print(f"   âš ï¸ No answer found in response")
                return any(kw in title for kw in keywords) and 'é¦™æ¸¯' in title
        elif response.status_code == 401 or response.status_code == 403:
            print(f"   âŒ API auth failed (status {response.status_code})")
            return any(kw in title for kw in keywords) and 'é¦™æ¸¯' in title
        else:
            print(f"   âš ï¸ API error: {response.status_code}")
            return any(kw in title for kw in keywords) and 'é¦™æ¸¯' in title
        
    except Exception as e:
        print(f"   âŒ AI check failed: {e}")
        return any(kw in title for kw in keywords) and 'é¦™æ¸¯' in title

    return any(kw in title for kw in keywords)

def parse_rss_source(name, url, sent_articles):
    """Parse RSS/JSON source and return matching articles"""
    articles = []
    now = datetime.datetime.now(HK_TZ)
    
    try:
        if 'news.google.com' in url:
            feed = feedparser.parse(url)
            for entry in feed.entries[:30]:
                link = entry.link
                
                # Skip if already sent
                if link in sent_articles:
                    print(f"   â­ï¸ Already sent, skipping: {entry.title[:40]}...")
                    continue
                
                if check_with_minimax(entry.title, name):
                    time_struct = getattr(entry, 'published_parsed', None)
                    if time_struct:
                        dt_obj = datetime.datetime.fromtimestamp(
                            time.mktime(time_struct), HK_TZ
                        )
                        # Only today's news
                        if is_today(dt_obj):
                            articles.append({
                                'source': name,
                                'title': entry.title.rsplit(' - ', 1)[0],
                                'link': link,
                                'datetime': dt_obj
                            })
        elif 'wenweipo.com' in url:
            response = requests.get(url, timeout=15)
            data = response.json()
            for item in data.get('data', [])[:30]:
                title = item.get('title', '')
                link = item.get('url', '')
                
                # Skip if already sent
                if link in sent_articles:
                    print(f"   â­ï¸ Already sent, skipping: {title[:40]}...")
                    continue
                
                if check_with_minimax(title, 'æ–‡åŒ¯å ±'):
                    pub_date = item.get('publishTime') or item.get('updated')
                    if pub_date:
                        try:
                            dt_obj = datetime.datetime.strptime(
                                pub_date, "%Y-%m-%dT%H:%M:%S.%f%z"
                            )
                            dt_obj = dt_obj.astimezone(HK_TZ)
                            # Only today's news
                            if is_today(dt_obj):
                                articles.append({
                                    'source': 'æ–‡åŒ¯å ±',
                                    'title': title,
                                    'link': link,
                                    'datetime': dt_obj
                                })
                        except:
                            pass
        else:
            response = requests.get(url, timeout=15)
            feed = feedparser.parse(response.content)
            for entry in feed.entries[:30]:
                link = entry.link
                
                # Skip if already sent
                if link in sent_articles:
                    print(f"   â­ï¸ Already sent, skipping: {entry.title[:40]}...")
                    continue
                
                if check_with_minimax(entry.title, name):
                    time_struct = getattr(entry, 'updated_parsed', None) or getattr(entry, 'published_parsed', None)
                    if time_struct:
                        dt_obj = datetime.datetime.fromtimestamp(
                            time.mktime(time_struct), HK_TZ
                        )
                        # Only today's news
                        if is_today(dt_obj):
                            articles.append({
                                'source': name,
                                'title': entry.title.rsplit(' - ', 1)[0],
                                'link': link,
                                'datetime': dt_obj
                            })
    except Exception as e:
        print(f"âŒ Error fetching {name}: {e}")
    
    return articles

def main():
    print(f"\nğŸ• [{datetime.datetime.now(HK_TZ).strftime('%Y-%m-%d %H:%M:%S')}] Starting AI news monitor...")
    print(f"ğŸ“¡ Monitoring {len(RSS_SOURCES)} sources")
    
    # Load already sent articles
    sent_articles = load_sent_articles()
    print(f"ğŸ“‹ Loaded {len(sent_articles)} previously sent articles")
    
    # Check API key
    api_key = os.environ.get('MINIMAX_API_KEY', '')
    if api_key:
        print(f"ğŸ¤– MiniMax API key: {api_key[:10]}...")
    else:
        print("âš ï¸ MINIMAX_API_KEY not set, using keyword fallback")
    
    print()
    
    all_articles = []
    
    for name, url in RSS_SOURCES.items():
        print(f"ğŸ“¥ Fetching {name}...")
        articles = parse_rss_source(name, url, sent_articles)
        all_articles.extend(articles)
        print(f"   â†’ Found {len(articles)} new articles")
    
    # Sort by time
    all_articles.sort(key=lambda x: x['datetime'], reverse=True)
    
    # Remove duplicates (by title within this run)
    seen = set()
    unique_articles = []
    for article in all_articles:
        title_key = article['title'][:30]
        if title_key not in seen:
            seen.add(title_key)
            unique_articles.append(article)
    
    # Group by source
    articles_by_source = {}
    for article in unique_articles:
        source = article['source']
        if source not in articles_by_source:
            articles_by_source[source] = []
        articles_by_source[source].append(article)
    
    print(f"\nğŸ“Š New articles by source: {dict((k, len(v)) for k, v in articles_by_source.items())}")
    
    # Send notification if there are new articles
    if unique_articles:
        message = "ğŸ“° ç¶œåˆåª’é«”å¿«è¨Š (å½™æ•´)\n\n"
        
        for source, articles in articles_by_source.items():
            emoji_map = {
                'æ”¿åºœæ–°è': 'ğŸ“°',
                'HK01': 'ğŸ“°',
                'on.cc': 'ğŸ“°',
                'nowæ–°è': 'ğŸ“°',
                'ç¦æ¯’/æµ·é—œ': 'ğŸ“°',
                'RTHK': 'ğŸ“°',
                'æ˜Ÿå³¶': 'ğŸ¯',
                'æ˜å ±': 'ğŸ“',
                'æ–‡åŒ¯å ±': 'ğŸ“°',
            }
            emoji = emoji_map.get(source, 'ğŸ“°')
            message += f"{emoji} {source}\n"
            for article in articles[:5]:  # Max 5 per source
                title = article['title'].replace('\n', ' ').strip()
                message += f"â€¢ [{title}]({article['link']})\n"
            message += "\n"
        
        message += f"ğŸ”— https://github.com/aaronkwok0551/newschannel"
        
        # Only send if within monitoring hours (8am-7pm)
        now_hkt = datetime.datetime.now(HK_TZ)
        if 8 <= now_hkt.hour <= 19:
            if send_telegram(message):
                # Mark these articles as sent
                for article in unique_articles:
                    sent_articles.add(article['link'])
                save_sent_articles(sent_articles)
                print(f"\nâœ… Sent {len(unique_articles)} new articles, updated tracking file")
                
                # Commit tracking file for persistence across runs
                try:
                    import subprocess
                    subprocess.run(['git', 'add', SENT_ARTICLES_FILE], check=True)
                    subprocess.run(['git', 'config', 'user.name', 'Patrick AI'], check=True)
                    subprocess.run(['git', 'config', 'user.email', 'patrick@openclaw.ai'], check=True)
                    subprocess.run(['git', 'commit', '-m', 'Update sent articles tracking'], check=True)
                    subprocess.run(['git', 'push'], check=True)
                    print(f"   ğŸ“ Tracking file committed to repo")
                except Exception as e:
                    print(f"   âš ï¸ Could not commit tracking file: {e}")
            else:
                print(f"\nâš ï¸ Telegram send failed")
        else:
            print(f"\nâ° Outside monitoring hours (8am-7pm), notification skipped")
            # Still update tracking to avoid duplicate notifications next time
            for article in unique_articles:
                sent_articles.add(article['link'])
            save_sent_articles(sent_articles)
    else:
        print("\nğŸ“­ No new articles found today")
    
    print(f"\nâœ… Monitor complete at {datetime.datetime.now(HK_TZ).strftime('%Y-%m-%d %H:%M:%S')}")

if __name__ == '__main__':
    main()
